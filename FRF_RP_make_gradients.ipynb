{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import xlrd\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from pandas import DataFrame\n",
    "import scipy as sc\n",
    "from scipy import io\n",
    "from scipy.stats import pearsonr\n",
    "from os.path import join, exists, dirname\n",
    "from glob import glob\n",
    "from brainspace import gradient\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo = pd.read_excel(join(path_data, 'abide_func_fn_abideII_sorted.xlsx'), sheet_name='Sheet1', skiprows=0)\n",
    "demo = pd.read_excel(join(path_data, 'abide_func_fn_abideII_sorted.xlsx'), sheet_name='male', skiprows=0)\n",
    "\n",
    "\n",
    "sub_id = demo['ID']\n",
    "sub_list = demo['FILE_ID']\n",
    "label2 = demo['Group']\n",
    "label = demo['DX_GROUP']\n",
    "site_id = demo['Site']\n",
    "site_label = demo['SITE_Label']\n",
    "Age = demo['Age']\n",
    "FD = demo['MeanFD_Jenkinson'] # = func_mean_fd\n",
    "FIQ = demo['FIQ']\n",
    "sex = demo['Sex']\n",
    "\n",
    "male_index = np.where(sex=='M')[0]\n",
    "\n",
    "ASD_index = np.where(label == 1)[0]                \n",
    "TD_index = np.where(label == 2)[0]\n",
    "Total_index = np.concatenate((ASD_index,TD_index)) \n",
    "sorted_idx = np.concatenate((ASD_index,TD_index), axis = 0)\n",
    "\n",
    "site_label_sorted = np.array(site_label)[sorted_idx]\n",
    "\n",
    "print('Total : ', len(label))\n",
    "print('ASD : ', len(ASD_index))\n",
    "print('TD : ', len(TD_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels as sm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "i = Age # Age\n",
    "\n",
    "print('Mean : ', np.array(i)[ASD_index].mean(), 'Std : ', np.array(i)[ASD_index].std())\n",
    "print('Mean : ', np.array(i)[TD_index].mean(), 'Std : ', np.array(i)[TD_index].std())\n",
    "\n",
    "[s,p] = sc.stats.ttest_ind(np.array(i)[ASD_index],np.array(i)[TD_index], equal_var=False, axis=0) \n",
    "\n",
    "p_fdr = sm.stats.multitest.multipletests(p,alpha=0.05,method='fdr_bh')\n",
    "\n",
    "print('')\n",
    "print('t : ',s)\n",
    "print(\"p-value\")\n",
    "print('0 : ', np.where(p_fdr[0]==True),'\\n', np.where(p_fdr[0]==True)[0].shape)\n",
    "print(p_fdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Age among sites\n",
    "\n",
    "i = Age # Age\n",
    "\n",
    "label_index = TD_index # ASD_index TD_index Total_index\n",
    "\n",
    "index_1 = np.intersect1d(np.where(np.array(site_id)=='TCD')[0],ASD_index) # ASD_index TD_index Total_index label_index\n",
    "index_2 = np.intersect1d(np.where(np.array(site_id)=='NYU')[0],ASD_index)\n",
    "index_3 = np.intersect1d(np.where(np.array(site_id)=='IP')[0],ASD_index)\n",
    "\n",
    "index_4 = np.intersect1d(np.where(np.array(site_id)=='TCD')[0],TD_index) # ASD_index TD_index Total_index label_index\n",
    "index_5 = np.intersect1d(np.where(np.array(site_id)=='NYU')[0],TD_index)\n",
    "index_6 = np.intersect1d(np.where(np.array(site_id)=='IP')[0],TD_index)\n",
    "\n",
    "print('Mean : ', np.array(i)[index_1].mean(), 'Std : ', np.array(i)[index_1].std())\n",
    "print('Mean : ', np.array(i)[index_2].mean(), 'Std : ', np.array(i)[index_2].std())\n",
    "print('Mean : ', np.array(i)[index_3].mean(), 'Std : ', np.array(i)[index_3].std())\n",
    "\n",
    "print('Mean : ', np.array(i)[index_4].mean(), 'Std : ', np.array(i)[index_4].std())\n",
    "print('Mean : ', np.array(i)[index_5].mean(), 'Std : ', np.array(i)[index_5].std())\n",
    "print('Mean : ', np.array(i)[index_6].mean(), 'Std : ', np.array(i)[index_6].std())\n",
    "\n",
    "print('TCD : ', len(index_1), ' ', len(index_4))\n",
    "print('NYU : ', len(index_2), ' ', len(index_5))\n",
    "print('IP : ', len(index_3), ' ', len(index_6))\n",
    "\n",
    "[s1,p1] = sc.stats.ttest_ind(np.array(i)[index_1],np.array(i)[index_4], equal_var=False, axis=0) \n",
    "[s2,p2] = sc.stats.ttest_ind(np.array(i)[index_2],np.array(i)[index_5], equal_var=False, axis=0) \n",
    "[s3,p3] = sc.stats.ttest_ind(np.array(i)[index_3],np.array(i)[index_6], equal_var=False, axis=0) \n",
    "\n",
    "\n",
    "p1_fdr = sm.stats.multitest.multipletests(p1,alpha=0.05,method='fdr_bh')\n",
    "p2_fdr = sm.stats.multitest.multipletests(p2,alpha=0.05,method='fdr_bh')\n",
    "p3_fdr = sm.stats.multitest.multipletests(p3,alpha=0.05,method='fdr_bh')\n",
    "\n",
    "print('')\n",
    "print(\"p-value\")\n",
    "print('p1 : ', np.where(p1_fdr[0]==True),'\\n', np.where(p1_fdr[0]==True)[0].shape)\n",
    "print(p1_fdr)\n",
    "\n",
    "print('p2 : ', np.where(p2_fdr[0]==True),'\\n', np.where(p2_fdr[0]==True)[0].shape)\n",
    "print(p2_fdr)\n",
    "\n",
    "print('p3 : ', np.where(p3_fdr[0]==True),'\\n', np.where(p3_fdr[0]==True)[0].shape)\n",
    "print(p3_fdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADOS_Total = np.nan_to_num(np.array(demo['ADOS_G_TOTAL']), nan = -1e-16) # -9999 -> -1, nan -> -2\n",
    "ADOS_Total = np.where(ADOS_Total == -999, -1e-16, ADOS_Total)\n",
    "ADOS_Total\n",
    "\n",
    "ADOS_comm = np.nan_to_num(np.array(demo['ADOS_G_COMM']), nan = -1e-16)\n",
    "ADOS_comm = np.where(ADOS_comm == -999, -1e-16, ADOS_comm)\n",
    "\n",
    "ADOS_social =  np.nan_to_num(np.array(demo['ADOS_G_SOCIAL']), nan = -1e-16)\n",
    "ADOS_social = np.where(ADOS_social == -999, -1e-16, ADOS_social)\n",
    "\n",
    "ADOS_behav =  np.nan_to_num(np.array(demo['ADOS_G_STEREO_BEHAV']), nan = -1e-16)\n",
    "ADOS_behav = np.where(ADOS_behav == -999, -1e-16, ADOS_behav)\n",
    "\n",
    "for i in [ADOS_Total, ADOS_comm, ADOS_social, ADOS_behav]:\n",
    "\n",
    "    print('Total Mean : ', np.array(i)[ASD_index].mean(), 'Std : ', np.array(i)[ASD_index].std()) # ASD_index index_1 index_2 index_3\n",
    "    print('TCD Mean : ', np.array(i)[index_1].mean(), 'Std : ', np.array(i)[index_1].std()) # ASD_index index_1 index_2 index_3\n",
    "    print('NYU Mean : ', np.array(i)[index_2].mean(), 'Std : ', np.array(i)[index_2].std()) # ASD_index index_1 index_2 index_3\n",
    "    print('IP Mean : ', np.array(i)[index_3].mean(), 'Std : ', np.array(i)[index_3].std()) # ASD_index index_1 index_2 index_3\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make functional gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make surface based pearson connectivity matrtix\n",
    "# ABIDE 2\n",
    "\n",
    "save_ts = False\n",
    "\n",
    "file_list = glob(join(path_ts,'*npy'))\n",
    "\n",
    "demo = pd.read_excel(join(path_ts_demo, 'abide_func_fn_abideII.xlsx'), sheet_name='abide_func_fn_abideII', skiprows=0)\n",
    "\n",
    "ID = demo['ID']\n",
    "# OrgID = demo['OrgID']\n",
    "Site = demo['Site']\n",
    "\n",
    "pcor = ConnectivityMeasure(kind ='correlation') # partial correlation correlation\n",
    "\n",
    "for i, filename in enumerate(file_list):\n",
    "    print(i)\n",
    "    Site_num = np.array(Site)[i]\n",
    "    ID_num = np.array(ID)[i]\n",
    "    folder_name = str(Site_num) + '_' + str(ID_num)\n",
    "#     if not(os.path.isdir(join(path_work,f'{folder_name}'))):\n",
    "#         print(f'Make folder {folder_name}')\n",
    "#         os.makedirs(os.path.join(path_work,f'{folder_name}'))\n",
    "#     if os.path.isdir(join(path_work,f'{folder_name}')):\n",
    "#         print('pass')\n",
    "#     else:\n",
    "#         os.makedirs(os.path.join(path_work,f'{folder_name}'))\n",
    "\n",
    "    save_path = glob(join(path_work,f'*{ID_num}'))\n",
    "    print(ID_num , ' ', Site_num, ' ', save_path)\n",
    "    \n",
    "    ts = np.load(filename)\n",
    "    n_roi = len(ts)\n",
    "    \n",
    "    if save_ts:\n",
    "        print(ts.shape)\n",
    "        np.save(join(path_work, f'{folder_name}/ts_MMP.npy'), ts)\n",
    "        print('save ts')\n",
    "\n",
    "    conn_mat = pcor.fit_transform(ts.T.reshape(1,-1,360))[0]\n",
    "    conn_mat = np.where(np.eye(n_roi) ==1, 0, conn_mat)\n",
    "#     print(conn_mat)\n",
    "    \n",
    "#     np.save(join(save_path[0], 'surf_conn_mat.npy'), conn_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combat harmonization correlation matrix\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure, sym_matrix_to_vec, vec_to_sym_matrix\n",
    "import neuroCombat\n",
    "\n",
    "file_list = [join(path_data,f'{workfolder}',f'{sub_list[i]}','surf_conn_mat.npy') for i in range(len(sub_list))]\n",
    "list_aff = [np.load(sl) for sl in file_list]\n",
    "\n",
    "n = len(list_aff)\n",
    "\n",
    "list_aff_vec = [None] * n\n",
    "\n",
    "for i, x1 in enumerate(list_aff):\n",
    "    print(i,' ', end = '', flush = True)\n",
    "    x1 = x1 + np.eye(len(x1))\n",
    "    list_aff_vec[i] = sym_matrix_to_vec(x1)\n",
    "\n",
    "site_sorted = site_id[sorted_idx]\n",
    "siteID = np.array([list(site_sorted).index(x) for x in site_sorted])\n",
    "\n",
    "covars = pd.DataFrame({'Site' : siteID})\n",
    "combat_res = neuroCombat.neuroCombat(dat = np.array(list_aff_vec).T, covars = covars, batch_col = 'Site', ref_batch = None) # list(site_sorted).index(ref_site))\n",
    "\n",
    "surf_conn_mat_har = [None] * n\n",
    "\n",
    "for i, x1 in enumerate(combat_res.T):\n",
    "    print(i,' ', end = '', flush = True)\n",
    "    \n",
    "    x2 = vec_to_sym_matrix(x1)\n",
    "    x2 = np.where(np.eye(len(x2)) ==1, 0, x2)\n",
    "    \n",
    "    surf_conn_mat_har[i] = x2\n",
    "\n",
    "for i, filename in enumerate(file_list):\n",
    "    print(i)\n",
    "    FileID_num = np.array(sub_list)[i]\n",
    "    save_path = glob(join(path_work,f'*{FileID_num}'))\n",
    "    print(FileID_num, ' ', save_path)\n",
    "      \n",
    "#     np.save(join(save_path[0], 'surf_conn_mat_Mean_har_male.npy'), np.array(surf_conn_mat_har[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC thresholding\n",
    "\n",
    "file_list = [join(path_data,f'{workfolder}',f'{sub_list[i]}','surf_conn_mat_Mean_har.npy') for i in range(len(sub_list))]\n",
    "path_work = join(path_data,f'{workfolder}')\n",
    "\n",
    "for thresh in [0.99, 0.97, 0.95, 0.9, 0.75, 0.5]:\n",
    "\n",
    "    sparsity = thresh\n",
    "\n",
    "    k = str(int(100-sparsity*100))\n",
    "    k = k.zfill(2)\n",
    "\n",
    "    print(f'Top {k}')\n",
    "\n",
    "    for i, x in enumerate(file_list):\n",
    "        print(f'{i} / {len(file_list)}')\n",
    "        conn_mat = np.load(x)\n",
    "        z_conn_mat = np.arctanh(np.nan_to_num(conn_mat, nan=0.0))\n",
    "        noaff_conn_mat = gradient.compute_affinity(z_conn_mat, sparsity=sparsity)\n",
    "        aff_conn_mat = gradient.compute_affinity(z_conn_mat, kernel = 'cosine', sparsity = sparsity)\n",
    "        \n",
    "        print(f'noaff isnan : {np.isnan(noaff_conn_mat).sum()}, symmetric : {gradient.is_symmetric(noaff_conn_mat)}')\n",
    "        print(f'aff isnan : {np.isnan(aff_conn_mat).sum()}, symmetric : {gradient.is_symmetric(aff_conn_mat)}')\n",
    "        \n",
    "#         np.save(join(path_work, x.split('\\\\')[-2], f'noaff_surf_mat_Mean_har_top{k}.npy'),noaff_conn_mat)\n",
    "#         np.save(join(path_work, x.split('\\\\')[-2], f'aff_surf_mat_Mean_har_top{k}.npy'), aff_conn_mat)\n",
    "\n",
    "    # grpmean\n",
    "    subj_num = len(file_list)\n",
    "\n",
    "    list_aff = [np.load(sl) for sl in file_list]\n",
    "    grpmean_conn_mat = np.nan_to_num(np.arctanh(np.nan_to_num(np.mean(list_aff, axis = 0), nan=0.0)), nan = 0.0)\n",
    "\n",
    "    noaff_grpmean_conn_mat = gradient.compute_affinity(grpmean_conn_mat, sparsity=sparsity) # sparsity로 thresholding ratio 조절\n",
    "    noaff_grpmean_conn_mat = np.nan_to_num(noaff_grpmean_conn_mat,nan=0.0)\n",
    "    aff_grpmean_conn_mat = gradient.compute_affinity(grpmean_conn_mat, kernel = 'cosine', sparsity=sparsity)\n",
    "    aff_grpmean_conn_mat = np.nan_to_num(aff_grpmean_conn_mat,nan=0.0)\n",
    "\n",
    "    # np.save(join(path_work, f'grp_mean.noaff_surf_mat_Mean_har_n{subj_num}_top{k}.npy'), noaff_grpmean_conn_mat)\n",
    "    # np.save(join(path_work, f'grp_mean.aff_surf_mat_Mean_har_n{subj_num}_top{k}.npy'), aff_grpmean_conn_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction\n",
    "# PCA alignment, DiffuisonMap alignment\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "embedding = 'PCA' #  'PCA', 'Diffusion'\n",
    "\n",
    "comp_num = 10\n",
    "\n",
    "if embedding == 'Diffusion':\n",
    "    file_list = []\n",
    "    for i in range(len(sub_list)):\n",
    "        file_list.append(glob(join(path_data,f'{workfolder}',f'{sub_list[i]}','aff*'))) \n",
    "    file_list = np.array(file_list).reshape(-1)\n",
    "    print(len(file_list))\n",
    "    list_aff = [np.load(sl) for sl in file_list]\n",
    "    grp_aff = np.load(join(path_work, f'grp_mean.aff_mat_n{len(file_list)}.npy')) \n",
    "    emb = gradient.embedding.DiffusionMaps(n_components = comp_num) \n",
    "    print(emb)\n",
    "\n",
    "elif embedding == 'PCA':\n",
    "    file_list = []\n",
    "    for i in range(len(sub_list)):\n",
    "        file_list.append(glob(join(path_data,f'{workfolder}',f'{sub_list[i]}','noaff_surf_mat_Mean_har_top50.npy'))) \n",
    "    file_list = np.array(file_list).reshape(-1)\n",
    "    print(len(file_list))\n",
    "    list_aff = [np.load(sl) for sl in file_list] \n",
    "    grp_aff = np.load(join(path_work,  f'grp_mean.noaff_surf_mat_Mean_har_n211_top50.npy'))  \n",
    "\n",
    "    emb = gradient.embedding.PCAMaps(n_components = comp_num) \n",
    "    print(emb)\n",
    "    \n",
    "# make referece\n",
    "emb.fit(grp_aff)\n",
    "ref_lam = emb.lambdas_ \n",
    "ref_PC = emb.maps_ \n",
    "\n",
    "n = len(list_aff)\n",
    "print(len(list_aff))\n",
    "lam, grad, vec = [None] * n, [None] * n, [None] * n\n",
    "for i, x1 in enumerate(list_aff):\n",
    "    print(i,' ', end = '', flush = True)\n",
    "    emb.fit(x1)\n",
    "    lam[i], grad[i] = emb.lambdas_ , emb.maps_\n",
    "\n",
    "pa = gradient.ProcrustesAlignment(n_iter=10)\n",
    "pa.fit(grad, reference=ref_PC)\n",
    "aligned = np.array(pa.aligned_)\n",
    "\n",
    "# np.save(f'{path_work}/noaff_surf_mat_Mean_har_n{len(file_list)}_gradients_top50',aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise regressed out\n",
    "\n",
    "save = False\n",
    "\n",
    "subj_num = 103\n",
    "thresh = 'top50' # top01 top03 top05 top10 top25 top30 top50 nothresh \n",
    "\n",
    "for i in [0,1,2]:\n",
    "\n",
    "    pc_num = i\n",
    "    data = f'noaff_surf_mat_Mean_har_n{subj_num}_gradients_{thresh}.npy'\n",
    "    gradients = np.load(join(path_work,data))\n",
    "    grad_num = pc_num + 1\n",
    "    ind_PCs = gradients[:,:,pc_num]\n",
    "\n",
    "    import statsmodels as sm\n",
    "    from statsmodels.api import GLM\n",
    "\n",
    "    list_reg_out = []\n",
    "    for i in range(ind_PCs.shape[1]):\n",
    "        print(i, ' ', end='', flush=True)\n",
    "        a = GLM(ind_PCs[:,i],sm.api.add_constant(np.array([Age,FD,Age*FD]).T))\n",
    "\n",
    "        res = a.fit()\n",
    "        # res.summary()\n",
    "\n",
    "        list_reg_out.append(res.resid_response)\n",
    "\n",
    "    ind_PCs_reg_out = np.array(list_reg_out).T\n",
    "\n",
    "    grp_ASD_PCs_reg_out = ind_PCs_reg_out[ASD_index,:]\n",
    "    grp_TD_PCs_reg_out = ind_PCs_reg_out[TD_index,:]\n",
    "\n",
    "    save = False\n",
    "    PCs_sorted_reg_out = np.concatenate((grp_ASD_PCs_reg_out,grp_TD_PCs_reg_out), axis = 0)\n",
    "\n",
    "    # print(pc_num)\n",
    "\n",
    "    if save:\n",
    "        print('save')\n",
    "        np.save(join(path_data,f'Gradient{grad_num}_Surf_pearson_sorted_regout_gender_Mean_har_n{subj_num}_{thresh}_ABD2.npy'), PCs_sorted_reg_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Feature Selction file (ABIDE 1 Feature using)\n",
    "\n",
    "subj_num = 103 # 103, 93\n",
    "thresh = 'top50' # nothresh top10 top25 top30 top50\n",
    "pc_num = 0\n",
    "grad_num = pc_num + 1\n",
    "\n",
    "file = f'Gradient{grad_num}_Surf_pearson_sorted_regout_gender_Mean_har_n{subj_num}_{thresh}_ABD2.npy'\n",
    "\n",
    "PCs_sorted_reg_out = np.load(join(path_data,file))\n",
    "label_sorted = np.concatenate((np.array(label[ASD_index]), np.array(label[TD_index])), axis = 0)\n",
    "sorted_idx = np.concatenate((ASD_index,TD_index), axis = 0)\n",
    "\n",
    "feature = np.load(join(path_data,f'G{grad_num}_{thresh}_feature_ABD1.npy'))\n",
    "\n",
    "save_name = f'Gradient{grad_num}_Surf_pearson_sorted_regout_gender_Mean_har_n{subj_num}_{thresh}_ABD2_FeaSel_Valid.mat'\n",
    "\n",
    "np.save(join(path_data,f'Gradient{grad_num}_Surf_pearson_sorted_regout_gender_Mean_har_n{subj_num}_{thresh}_ABD2_FeaSel_Valid.npy'), PCs_sorted_reg_out[:,feature])\n",
    "sc.io.savemat(join(path_data,save_name), {'total_group_data': np.concatenate((PCs_sorted_reg_out[:,feature],label_sorted.reshape(-1,1)),axis=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Funtaional Random Forest algorithm in matlab using save file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gudtls17_py36",
   "language": "python",
   "name": "gudtls17_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
