{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import xlrd\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from pandas import DataFrame\n",
    "import scipy as sc\n",
    "from scipy import io\n",
    "from scipy.stats import pearsonr\n",
    "from os.path import join, exists, dirname\n",
    "from glob import glob\n",
    "from brainspace import gradient\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data (업로드시 제외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path\n",
    "path_data = join('V:/', 'hschoi/1.asd.grad\\\\data')\n",
    "workfolder = 'ABIDE_2'\n",
    "path_work = join(path_data,workfolder)\n",
    "path_ts = 'W:\\\\ksbyeon\\\\1.MIPL\\\\21.DNN.autism\\\\1.data\\\\Hong\\\\work\\\\prep-ABIDE-II\\\\MMP_tsmean\\\\'\n",
    "path_ts_demo = 'W:\\\\ksbyeon\\\\1.MIPL\\\\21.DNN.autism\\\\1.data\\\\Hong\\\\ABIDE_fMRI'\n",
    "path_mat = 'V:/hschoi/1.asd.grad/code/matlab_code/functional-random-forest-master/RFSD/'\n",
    "atlas_path = join('V:/hschoi/template')\n",
    "path_gii_data = 'V:/hschoi/1.asd.grad/code/surface/npy2gii/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total :  86\n",
      "ASD :  44\n",
      "TD :  42\n"
     ]
    }
   ],
   "source": [
    "# demo = pd.read_excel(join(path_data, 'abide_func_fn_abideII_sorted.xlsx'), sheet_name='Sheet1', skiprows=0)\n",
    "demo = pd.read_excel(join(path_data, 'abide_func_fn_abideII_sorted.xlsx'), sheet_name='male', skiprows=0)\n",
    "\n",
    "\n",
    "sub_id = demo['ID']\n",
    "sub_list = demo['FILE_ID']\n",
    "label2 = demo['Group']\n",
    "label = demo['DX_GROUP']\n",
    "site_id = demo['Site']\n",
    "site_label = demo['SITE_Label']\n",
    "Age = demo['Age']\n",
    "FD = demo['MeanFD_Jenkinson'] # = func_mean_fd\n",
    "FIQ = demo['FIQ']\n",
    "sex = demo['Sex']\n",
    "\n",
    "male_index = np.where(sex=='M')[0]\n",
    "\n",
    "ASD_index = np.where(label == 1)[0]                \n",
    "TD_index = np.where(label == 2)[0]\n",
    "Total_index = np.concatenate((ASD_index,TD_index)) \n",
    "sorted_idx = np.concatenate((ASD_index,TD_index), axis = 0)\n",
    "\n",
    "site_label_sorted = np.array(site_label)[sorted_idx]\n",
    "\n",
    "print('Total : ', len(label))\n",
    "print('ASD : ', len(ASD_index))\n",
    "print('TD : ', len(TD_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean :  12.689363636363636 Std :  5.205407097392746\n",
      "Mean :  14.112952380952382 Std :  5.618645040663321\n",
      "\n",
      "t :  -1.2030981785198454\n",
      "p-value\n",
      "0 :  (array([], dtype=int64),) \n",
      " (0,)\n",
      "(array([False]), array([0.23236985]), 0.050000000000000044, 0.05)\n"
     ]
    }
   ],
   "source": [
    "import statsmodels as sm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "i = Age # Age\n",
    "\n",
    "print('Mean : ', np.array(i)[ASD_index].mean(), 'Std : ', np.array(i)[ASD_index].std())\n",
    "print('Mean : ', np.array(i)[TD_index].mean(), 'Std : ', np.array(i)[TD_index].std())\n",
    "\n",
    "[s,p] = sc.stats.ttest_ind(np.array(i)[ASD_index],np.array(i)[TD_index], equal_var=False, axis=0) \n",
    "\n",
    "p_fdr = sm.stats.multitest.multipletests(p,alpha=0.05,method='fdr_bh')\n",
    "\n",
    "print('')\n",
    "print('t : ',s)\n",
    "print(\"p-value\")\n",
    "print('0 : ', np.where(p_fdr[0]==True),'\\n', np.where(p_fdr[0]==True)[0].shape)\n",
    "print(p_fdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Age among sites\n",
    "\n",
    "i = Age # Age\n",
    "\n",
    "label_index = TD_index # ASD_index TD_index Total_index\n",
    "\n",
    "index_1 = np.intersect1d(np.where(np.array(site_id)=='TCD')[0],ASD_index) # ASD_index TD_index Total_index label_index\n",
    "index_2 = np.intersect1d(np.where(np.array(site_id)=='NYU')[0],ASD_index)\n",
    "index_3 = np.intersect1d(np.where(np.array(site_id)=='IP')[0],ASD_index)\n",
    "\n",
    "index_4 = np.intersect1d(np.where(np.array(site_id)=='TCD')[0],TD_index) # ASD_index TD_index Total_index label_index\n",
    "index_5 = np.intersect1d(np.where(np.array(site_id)=='NYU')[0],TD_index)\n",
    "index_6 = np.intersect1d(np.where(np.array(site_id)=='IP')[0],TD_index)\n",
    "\n",
    "print('Mean : ', np.array(i)[index_1].mean(), 'Std : ', np.array(i)[index_1].std())\n",
    "print('Mean : ', np.array(i)[index_2].mean(), 'Std : ', np.array(i)[index_2].std())\n",
    "print('Mean : ', np.array(i)[index_3].mean(), 'Std : ', np.array(i)[index_3].std())\n",
    "\n",
    "print('Mean : ', np.array(i)[index_4].mean(), 'Std : ', np.array(i)[index_4].std())\n",
    "print('Mean : ', np.array(i)[index_5].mean(), 'Std : ', np.array(i)[index_5].std())\n",
    "print('Mean : ', np.array(i)[index_6].mean(), 'Std : ', np.array(i)[index_6].std())\n",
    "\n",
    "print('TCD : ', len(index_1), ' ', len(index_4))\n",
    "print('NYU : ', len(index_2), ' ', len(index_5))\n",
    "print('IP : ', len(index_3), ' ', len(index_6))\n",
    "\n",
    "[s1,p1] = sc.stats.ttest_ind(np.array(i)[index_1],np.array(i)[index_4], equal_var=False, axis=0) \n",
    "[s2,p2] = sc.stats.ttest_ind(np.array(i)[index_2],np.array(i)[index_5], equal_var=False, axis=0) \n",
    "[s3,p3] = sc.stats.ttest_ind(np.array(i)[index_3],np.array(i)[index_6], equal_var=False, axis=0) \n",
    "\n",
    "\n",
    "p1_fdr = sm.stats.multitest.multipletests(p1,alpha=0.05,method='fdr_bh')\n",
    "p2_fdr = sm.stats.multitest.multipletests(p2,alpha=0.05,method='fdr_bh')\n",
    "p3_fdr = sm.stats.multitest.multipletests(p3,alpha=0.05,method='fdr_bh')\n",
    "\n",
    "print('')\n",
    "print(\"p-value\")\n",
    "print('p1 : ', np.where(p1_fdr[0]==True),'\\n', np.where(p1_fdr[0]==True)[0].shape)\n",
    "print(p1_fdr)\n",
    "\n",
    "print('p2 : ', np.where(p2_fdr[0]==True),'\\n', np.where(p2_fdr[0]==True)[0].shape)\n",
    "print(p2_fdr)\n",
    "\n",
    "print('p3 : ', np.where(p3_fdr[0]==True),'\\n', np.where(p3_fdr[0]==True)[0].shape)\n",
    "print(p3_fdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADOS_Total = np.nan_to_num(np.array(demo['ADOS_G_TOTAL']), nan = -1e-16) # -9999 -> -1, nan -> -2\n",
    "ADOS_Total = np.where(ADOS_Total == -999, -1e-16, ADOS_Total)\n",
    "ADOS_Total\n",
    "\n",
    "ADOS_comm = np.nan_to_num(np.array(demo['ADOS_G_COMM']), nan = -1e-16)\n",
    "ADOS_comm = np.where(ADOS_comm == -999, -1e-16, ADOS_comm)\n",
    "\n",
    "ADOS_social =  np.nan_to_num(np.array(demo['ADOS_G_SOCIAL']), nan = -1e-16)\n",
    "ADOS_social = np.where(ADOS_social == -999, -1e-16, ADOS_social)\n",
    "\n",
    "ADOS_behav =  np.nan_to_num(np.array(demo['ADOS_G_STEREO_BEHAV']), nan = -1e-16)\n",
    "ADOS_behav = np.where(ADOS_behav == -999, -1e-16, ADOS_behav)\n",
    "\n",
    "for i in [ADOS_Total, ADOS_comm, ADOS_social, ADOS_behav]:\n",
    "\n",
    "    print('Total Mean : ', np.array(i)[ASD_index].mean(), 'Std : ', np.array(i)[ASD_index].std()) # ASD_index index_1 index_2 index_3\n",
    "    print('TCD Mean : ', np.array(i)[index_1].mean(), 'Std : ', np.array(i)[index_1].std()) # ASD_index index_1 index_2 index_3\n",
    "    print('NYU Mean : ', np.array(i)[index_2].mean(), 'Std : ', np.array(i)[index_2].std()) # ASD_index index_1 index_2 index_3\n",
    "    print('IP Mean : ', np.array(i)[index_3].mean(), 'Std : ', np.array(i)[index_3].std()) # ASD_index index_1 index_2 index_3\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make functional gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make surface based pearson connectivity matrtix\n",
    "# ABIDE 2\n",
    "\n",
    "save_ts = False\n",
    "\n",
    "file_list = glob(join(path_ts,'*npy'))\n",
    "\n",
    "demo = pd.read_excel(join(path_ts_demo, 'abide_func_fn_abideII.xlsx'), sheet_name='abide_func_fn_abideII', skiprows=0)\n",
    "\n",
    "ID = demo['ID']\n",
    "# OrgID = demo['OrgID']\n",
    "Site = demo['Site']\n",
    "\n",
    "pcor = ConnectivityMeasure(kind ='correlation') # partial correlation correlation\n",
    "\n",
    "for i, filename in enumerate(file_list):\n",
    "    print(i)\n",
    "    Site_num = np.array(Site)[i]\n",
    "    ID_num = np.array(ID)[i]\n",
    "    folder_name = str(Site_num) + '_' + str(ID_num)\n",
    "#     if not(os.path.isdir(join(path_work,f'{folder_name}'))):\n",
    "#         print(f'Make folder {folder_name}')\n",
    "#         os.makedirs(os.path.join(path_work,f'{folder_name}'))\n",
    "#     if os.path.isdir(join(path_work,f'{folder_name}')):\n",
    "#         print('pass')\n",
    "#     else:\n",
    "#         os.makedirs(os.path.join(path_work,f'{folder_name}'))\n",
    "\n",
    "    save_path = glob(join(path_work,f'*{ID_num}'))\n",
    "    print(ID_num , ' ', Site_num, ' ', save_path)\n",
    "    \n",
    "    ts = np.load(filename)\n",
    "    n_roi = len(ts)\n",
    "    \n",
    "    if save_ts:\n",
    "        print(ts.shape)\n",
    "        np.save(join(path_work, f'{folder_name}/ts_MMP.npy'), ts)\n",
    "        print('save ts')\n",
    "\n",
    "    conn_mat = pcor.fit_transform(ts.T.reshape(1,-1,360))[0]\n",
    "    conn_mat = np.where(np.eye(n_roi) ==1, 0, conn_mat)\n",
    "#     print(conn_mat)\n",
    "    \n",
    "#     np.save(join(save_path[0], 'surf_conn_mat.npy'), conn_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  [neuroCombat] Creating design matrix\n",
      "[neuroCombat] Standardizing data across features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\woowon\\.conda\\envs\\gudtls17_py36\\lib\\site-packages\\neuroCombat\\neuroCombat.py:233: RuntimeWarning: invalid value encountered in true_divide\n",
      "  s_data = ((X- stand_mean) / np.dot(np.sqrt(var_pooled), np.ones((1, n_sample))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neuroCombat] Fitting L/S model and finding priors\n",
      "[neuroCombat] Finding parametric adjustments\n",
      "[neuroCombat] Final adjustment of data\n",
      "0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  0\n",
      "NYU_29189   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29189']\n",
      "1\n",
      "NYU_29180   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29180']\n",
      "2\n",
      "NYU_29202   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29202']\n",
      "3\n",
      "NYU_29193   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29193']\n",
      "4\n",
      "NYU_29184   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29184']\n",
      "5\n",
      "IP_29606   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29606']\n",
      "6\n",
      "IP_29602   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29602']\n",
      "7\n",
      "NYU_29185   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29185']\n",
      "8\n",
      "IP_29589   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29589']\n",
      "9\n",
      "IP_29580   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29580']\n",
      "10\n",
      "NYU_29215   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29215']\n",
      "11\n",
      "NYU_29218   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29218']\n",
      "12\n",
      "NYU_29208   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29208']\n",
      "13\n",
      "IP_29608   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29608']\n",
      "14\n",
      "IP_29583   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29583']\n",
      "15\n",
      "NYU_29188   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29188']\n",
      "16\n",
      "NYU_29210   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29210']\n",
      "17\n",
      "NYU_29181   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29181']\n",
      "18\n",
      "NYU_29199   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29199']\n",
      "19\n",
      "NYU_29220   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29220']\n",
      "20\n",
      "NYU_29191   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29191']\n",
      "21\n",
      "IP_29587   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29587']\n",
      "22\n",
      "TCD_29116   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29116']\n",
      "23\n",
      "NYU_29198   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29198']\n",
      "24\n",
      "IP_29594   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29594']\n",
      "25\n",
      "TCD_29128   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29128']\n",
      "26\n",
      "NYU_29196   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29196']\n",
      "27\n",
      "TCD_29127   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29127']\n",
      "28\n",
      "TCD_29133   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29133']\n",
      "29\n",
      "TCD_29099   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29099']\n",
      "30\n",
      "TCD_29112   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29112']\n",
      "31\n",
      "NYU_29197   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29197']\n",
      "32\n",
      "TCD_29109   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29109']\n",
      "33\n",
      "TCD_29113   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29113']\n",
      "34\n",
      "IP_29609   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29609']\n",
      "35\n",
      "NYU_29200   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29200']\n",
      "36\n",
      "TCD_29117   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29117']\n",
      "37\n",
      "TCD_29101   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29101']\n",
      "38\n",
      "TCD_29108   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29108']\n",
      "39\n",
      "TCD_29136   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29136']\n",
      "40\n",
      "TCD_29130   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29130']\n",
      "41\n",
      "TCD_29111   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29111']\n",
      "42\n",
      "TCD_29135   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29135']\n",
      "43\n",
      "TCD_29129   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29129']\n",
      "44\n",
      "TCD_29103   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29103']\n",
      "45\n",
      "TCD_29131   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29131']\n",
      "46\n",
      "TCD_29121   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29121']\n",
      "47\n",
      "IP_29629   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29629']\n",
      "48\n",
      "TCD_29120   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29120']\n",
      "49\n",
      "IP_29619   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29619']\n",
      "50\n",
      "TCD_29096   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29096']\n",
      "51\n",
      "IP_29615   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29615']\n",
      "52\n",
      "TCD_29107   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29107']\n",
      "53\n",
      "TCD_29114   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29114']\n",
      "54\n",
      "TCD_29124   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29124']\n",
      "55\n",
      "TCD_29122   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29122']\n",
      "56\n",
      "TCD_29123   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29123']\n",
      "57\n",
      "TCD_29125   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29125']\n",
      "58\n",
      "TCD_29105   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29105']\n",
      "59\n",
      "NYU_29250   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29250']\n",
      "60\n",
      "TCD_29119   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29119']\n",
      "61\n",
      "TCD_29104   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29104']\n",
      "62\n",
      "NYU_29249   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29249']\n",
      "63\n",
      "TCD_29118   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29118']\n",
      "64\n",
      "NYU_29243   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29243']\n",
      "65\n",
      "TCD_29115   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\TCD_29115']\n",
      "66\n",
      "IP_29588   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29588']\n",
      "67\n",
      "NYU_29246   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29246']\n",
      "68\n",
      "IP_29627   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29627']\n",
      "69\n",
      "NYU_29233   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29233']\n",
      "70\n",
      "IP_29614   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29614']\n",
      "71\n",
      "NYU_29237   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29237']\n",
      "72\n",
      "NYU_29252   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29252']\n",
      "73\n",
      "NYU_29251   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29251']\n",
      "74\n",
      "NYU_29235   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29235']\n",
      "75\n",
      "NYU_29231   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29231']\n",
      "76\n",
      "NYU_29203   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29203']\n",
      "77\n",
      "NYU_29230   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29230']\n",
      "78\n",
      "NYU_29241   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29241']\n",
      "79\n",
      "NYU_29227   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29227']\n",
      "80\n",
      "NYU_29232   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29232']\n",
      "81\n",
      "IP_29595   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\IP_29595']\n",
      "82\n",
      "NYU_29254   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29254']\n",
      "83\n",
      "NYU_29225   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29225']\n",
      "84\n",
      "NYU_29226   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29226']\n",
      "85\n",
      "NYU_29253   ['V:/hschoi/1.asd.grad\\\\data\\\\ABIDE_2\\\\NYU_29253']\n"
     ]
    }
   ],
   "source": [
    "# Combat harmonization correlation matrix\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure, sym_matrix_to_vec, vec_to_sym_matrix\n",
    "import neuroCombat\n",
    "\n",
    "file_list = [join(path_data,f'{workfolder}',f'{sub_list[i]}','surf_conn_mat.npy') for i in range(len(sub_list))]\n",
    "list_aff = [np.load(sl) for sl in file_list]\n",
    "\n",
    "n = len(list_aff)\n",
    "\n",
    "list_aff_vec = [None] * n\n",
    "\n",
    "for i, x1 in enumerate(list_aff):\n",
    "    print(i,' ', end = '', flush = True)\n",
    "    x1 = x1 + np.eye(len(x1))\n",
    "    list_aff_vec[i] = sym_matrix_to_vec(x1)\n",
    "\n",
    "site_sorted = site_id[sorted_idx]\n",
    "siteID = np.array([list(site_sorted).index(x) for x in site_sorted])\n",
    "\n",
    "covars = pd.DataFrame({'Site' : siteID})\n",
    "combat_res = neuroCombat.neuroCombat(dat = np.array(list_aff_vec).T, covars = covars, batch_col = 'Site', ref_batch = None) # list(site_sorted).index(ref_site))\n",
    "\n",
    "surf_conn_mat_har = [None] * n\n",
    "\n",
    "for i, x1 in enumerate(combat_res.T):\n",
    "    print(i,' ', end = '', flush = True)\n",
    "    \n",
    "    x2 = vec_to_sym_matrix(x1)\n",
    "    x2 = np.where(np.eye(len(x2)) ==1, 0, x2)\n",
    "    \n",
    "    surf_conn_mat_har[i] = x2\n",
    "\n",
    "for i, filename in enumerate(file_list):\n",
    "    print(i)\n",
    "    FileID_num = np.array(sub_list)[i]\n",
    "    save_path = glob(join(path_work,f'*{FileID_num}'))\n",
    "    print(FileID_num, ' ', save_path)\n",
    "      \n",
    "#     np.save(join(save_path[0], 'surf_conn_mat_Mean_har_male.npy'), np.array(surf_conn_mat_har[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC thresholding\n",
    "\n",
    "file_list = [join(path_data,f'{workfolder}',f'{sub_list[i]}','surf_conn_mat_Mean_har.npy') for i in range(len(sub_list))]\n",
    "path_work = join(path_data,f'{workfolder}')\n",
    "\n",
    "for thresh in [0.99, 0.97, 0.95, 0.9, 0.75, 0.5]:\n",
    "\n",
    "    sparsity = thresh\n",
    "\n",
    "    k = str(int(100-sparsity*100))\n",
    "    k = k.zfill(2)\n",
    "\n",
    "    print(f'Top {k}')\n",
    "\n",
    "    for i, x in enumerate(file_list):\n",
    "        print(f'{i} / {len(file_list)}')\n",
    "        conn_mat = np.load(x)\n",
    "        z_conn_mat = np.arctanh(np.nan_to_num(conn_mat, nan=0.0))\n",
    "        noaff_conn_mat = gradient.compute_affinity(z_conn_mat, sparsity=sparsity)\n",
    "        aff_conn_mat = gradient.compute_affinity(z_conn_mat, kernel = 'cosine', sparsity = sparsity)\n",
    "        \n",
    "        print(f'noaff isnan : {np.isnan(noaff_conn_mat).sum()}, symmetric : {gradient.is_symmetric(noaff_conn_mat)}')\n",
    "        print(f'aff isnan : {np.isnan(aff_conn_mat).sum()}, symmetric : {gradient.is_symmetric(aff_conn_mat)}')\n",
    "        \n",
    "#         np.save(join(path_work, x.split('\\\\')[-2], f'noaff_surf_mat_Mean_har_top{k}.npy'),noaff_conn_mat)\n",
    "#         np.save(join(path_work, x.split('\\\\')[-2], f'aff_surf_mat_Mean_har_top{k}.npy'), aff_conn_mat)\n",
    "\n",
    "    # grpmean\n",
    "    subj_num = len(file_list)\n",
    "\n",
    "    list_aff = [np.load(sl) for sl in file_list]\n",
    "    grpmean_conn_mat = np.nan_to_num(np.arctanh(np.nan_to_num(np.mean(list_aff, axis = 0), nan=0.0)), nan = 0.0)\n",
    "\n",
    "    noaff_grpmean_conn_mat = gradient.compute_affinity(grpmean_conn_mat, sparsity=sparsity) # sparsity로 thresholding ratio 조절\n",
    "    noaff_grpmean_conn_mat = np.nan_to_num(noaff_grpmean_conn_mat,nan=0.0)\n",
    "    aff_grpmean_conn_mat = gradient.compute_affinity(grpmean_conn_mat, kernel = 'cosine', sparsity=sparsity)\n",
    "    aff_grpmean_conn_mat = np.nan_to_num(aff_grpmean_conn_mat,nan=0.0)\n",
    "\n",
    "    # np.save(join(path_work, f'grp_mean.noaff_surf_mat_Mean_har_n{subj_num}_top{k}.npy'), noaff_grpmean_conn_mat)\n",
    "    # np.save(join(path_work, f'grp_mean.aff_surf_mat_Mean_har_n{subj_num}_top{k}.npy'), aff_grpmean_conn_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction\n",
    "# PCA alignment, DiffuisonMap alignment\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "embedding = 'PCA' #  'PCA', 'Diffusion'\n",
    "\n",
    "comp_num = 10\n",
    "\n",
    "if embedding == 'Diffusion':\n",
    "    file_list = []\n",
    "    for i in range(len(sub_list)):\n",
    "        file_list.append(glob(join(path_data,f'{workfolder}',f'{sub_list[i]}','aff*'))) \n",
    "    file_list = np.array(file_list).reshape(-1)\n",
    "    print(len(file_list))\n",
    "    list_aff = [np.load(sl) for sl in file_list]\n",
    "    grp_aff = np.load(join(path_work, f'grp_mean.aff_mat_n{len(file_list)}.npy')) \n",
    "    emb = gradient.embedding.DiffusionMaps(n_components = comp_num) \n",
    "    print(emb)\n",
    "\n",
    "elif embedding == 'PCA':\n",
    "    file_list = []\n",
    "    for i in range(len(sub_list)):\n",
    "        file_list.append(glob(join(path_data,f'{workfolder}',f'{sub_list[i]}','noaff_surf_mat_Mean_har_top50.npy'))) \n",
    "    file_list = np.array(file_list).reshape(-1)\n",
    "    print(len(file_list))\n",
    "    list_aff = [np.load(sl) for sl in file_list] \n",
    "    grp_aff = np.load(join(path_work,  f'grp_mean.noaff_surf_mat_Mean_har_n211_top50.npy'))  \n",
    "\n",
    "    emb = gradient.embedding.PCAMaps(n_components = comp_num) \n",
    "    print(emb)\n",
    "    \n",
    "# make referece\n",
    "emb.fit(grp_aff)\n",
    "ref_lam = emb.lambdas_ \n",
    "ref_PC = emb.maps_ \n",
    "\n",
    "n = len(list_aff)\n",
    "print(len(list_aff))\n",
    "lam, grad, vec = [None] * n, [None] * n, [None] * n\n",
    "for i, x1 in enumerate(list_aff):\n",
    "    print(i,' ', end = '', flush = True)\n",
    "    emb.fit(x1)\n",
    "    lam[i], grad[i] = emb.lambdas_ , emb.maps_\n",
    "\n",
    "pa = gradient.ProcrustesAlignment(n_iter=10)\n",
    "pa.fit(grad, reference=ref_PC)\n",
    "aligned = np.array(pa.aligned_)\n",
    "\n",
    "# np.save(f'{path_work}/noaff_surf_mat_Mean_har_n{len(file_list)}_gradients_top50',aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise regressed out\n",
    "\n",
    "save = False\n",
    "\n",
    "subj_num = 103\n",
    "thresh = 'top50' # top01 top03 top05 top10 top25 top30 top50 nothresh \n",
    "\n",
    "for i in [0,1,2]:\n",
    "\n",
    "    pc_num = i\n",
    "    data = f'noaff_surf_mat_Mean_har_n{subj_num}_gradients_{thresh}.npy'\n",
    "    gradients = np.load(join(path_work,data))\n",
    "    grad_num = pc_num + 1\n",
    "    ind_PCs = gradients[:,:,pc_num]\n",
    "\n",
    "    import statsmodels as sm\n",
    "    from statsmodels.api import GLM\n",
    "\n",
    "    list_reg_out = []\n",
    "    for i in range(ind_PCs.shape[1]):\n",
    "        print(i, ' ', end='', flush=True)\n",
    "        a = GLM(ind_PCs[:,i],sm.api.add_constant(np.array([Age,FD,Age*FD]).T))\n",
    "\n",
    "        res = a.fit()\n",
    "        # res.summary()\n",
    "\n",
    "        list_reg_out.append(res.resid_response)\n",
    "\n",
    "    ind_PCs_reg_out = np.array(list_reg_out).T\n",
    "\n",
    "    grp_ASD_PCs_reg_out = ind_PCs_reg_out[ASD_index,:]\n",
    "    grp_TD_PCs_reg_out = ind_PCs_reg_out[TD_index,:]\n",
    "\n",
    "    save = False\n",
    "    PCs_sorted_reg_out = np.concatenate((grp_ASD_PCs_reg_out,grp_TD_PCs_reg_out), axis = 0)\n",
    "\n",
    "    # print(pc_num)\n",
    "\n",
    "    if save:\n",
    "        print('save')\n",
    "        np.save(join(path_data,f'Gradient{grad_num}_Surf_pearson_sorted_regout_gender_Mean_har_n{subj_num}_{thresh}_ABD2.npy'), PCs_sorted_reg_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Feature Selction file (ABIDE 1 Feature using)\n",
    "\n",
    "subj_num = 103 # 103, 93\n",
    "thresh = 'top50' # nothresh top10 top25 top30 top50\n",
    "pc_num = 0\n",
    "grad_num = pc_num + 1\n",
    "\n",
    "file = f'Gradient{grad_num}_Surf_pearson_sorted_regout_gender_Mean_har_n{subj_num}_{thresh}_ABD2.npy'\n",
    "\n",
    "PCs_sorted_reg_out = np.load(join(path_data,file))\n",
    "label_sorted = np.concatenate((np.array(label[ASD_index]), np.array(label[TD_index])), axis = 0)\n",
    "sorted_idx = np.concatenate((ASD_index,TD_index), axis = 0)\n",
    "\n",
    "feature = np.load(join(path_data,f'G{grad_num}_{thresh}_feature_ABD1.npy'))\n",
    "\n",
    "save_name = f'Gradient{grad_num}_Surf_pearson_sorted_regout_gender_Mean_har_n{subj_num}_{thresh}_ABD2_FeaSel_Valid.mat'\n",
    "\n",
    "np.save(join(path_data,f'Gradient{grad_num}_Surf_pearson_sorted_regout_gender_Mean_har_n{subj_num}_{thresh}_ABD2_FeaSel_Valid.npy'), PCs_sorted_reg_out[:,feature])\n",
    "sc.io.savemat(join(path_data,save_name), {'total_group_data': np.concatenate((PCs_sorted_reg_out[:,feature],label_sorted.reshape(-1,1)),axis=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Funtaional Random Forest algorithm in matlab using save file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gudtls17_py36",
   "language": "python",
   "name": "gudtls17_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
