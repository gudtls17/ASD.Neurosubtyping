{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import xlrd\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from pandas import DataFrame\n",
    "import scipy as sc\n",
    "from scipy import io\n",
    "from scipy.stats import pearsonr\n",
    "from os.path import join, exists, dirname\n",
    "from glob import glob\n",
    "from brainspace import gradient\n",
    "from random import randint\n",
    "import nibabel as nib\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viscm not found, falling back on simple display\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add colormap parlula\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "cm_data = [[0.2081, 0.1663, 0.5292], [0.2116238095, 0.1897809524, 0.5776761905], \n",
    " [0.212252381, 0.2137714286, 0.6269714286], [0.2081, 0.2386, 0.6770857143], \n",
    " [0.1959047619, 0.2644571429, 0.7279], [0.1707285714, 0.2919380952, \n",
    "  0.779247619], [0.1252714286, 0.3242428571, 0.8302714286], \n",
    " [0.0591333333, 0.3598333333, 0.8683333333], [0.0116952381, 0.3875095238, \n",
    "  0.8819571429], [0.0059571429, 0.4086142857, 0.8828428571], \n",
    " [0.0165142857, 0.4266, 0.8786333333], [0.032852381, 0.4430428571, \n",
    "  0.8719571429], [0.0498142857, 0.4585714286, 0.8640571429], \n",
    " [0.0629333333, 0.4736904762, 0.8554380952], [0.0722666667, 0.4886666667, \n",
    "  0.8467], [0.0779428571, 0.5039857143, 0.8383714286], \n",
    " [0.079347619, 0.5200238095, 0.8311809524], [0.0749428571, 0.5375428571, \n",
    "  0.8262714286], [0.0640571429, 0.5569857143, 0.8239571429], \n",
    " [0.0487714286, 0.5772238095, 0.8228285714], [0.0343428571, 0.5965809524, \n",
    "  0.819852381], [0.0265, 0.6137, 0.8135], [0.0238904762, 0.6286619048, \n",
    "  0.8037619048], [0.0230904762, 0.6417857143, 0.7912666667], \n",
    " [0.0227714286, 0.6534857143, 0.7767571429], [0.0266619048, 0.6641952381, \n",
    "  0.7607190476], [0.0383714286, 0.6742714286, 0.743552381], \n",
    " [0.0589714286, 0.6837571429, 0.7253857143], \n",
    " [0.0843, 0.6928333333, 0.7061666667], [0.1132952381, 0.7015, 0.6858571429], \n",
    " [0.1452714286, 0.7097571429, 0.6646285714], [0.1801333333, 0.7176571429, \n",
    "  0.6424333333], [0.2178285714, 0.7250428571, 0.6192619048], \n",
    " [0.2586428571, 0.7317142857, 0.5954285714], [0.3021714286, 0.7376047619, \n",
    "  0.5711857143], [0.3481666667, 0.7424333333, 0.5472666667], \n",
    " [0.3952571429, 0.7459, 0.5244428571], [0.4420095238, 0.7480809524, \n",
    "  0.5033142857], [0.4871238095, 0.7490619048, 0.4839761905], \n",
    " [0.5300285714, 0.7491142857, 0.4661142857], [0.5708571429, 0.7485190476, \n",
    "  0.4493904762], [0.609852381, 0.7473142857, 0.4336857143], \n",
    " [0.6473, 0.7456, 0.4188], [0.6834190476, 0.7434761905, 0.4044333333], \n",
    " [0.7184095238, 0.7411333333, 0.3904761905], \n",
    " [0.7524857143, 0.7384, 0.3768142857], [0.7858428571, 0.7355666667, \n",
    "  0.3632714286], [0.8185047619, 0.7327333333, 0.3497904762], \n",
    " [0.8506571429, 0.7299, 0.3360285714], [0.8824333333, 0.7274333333, 0.3217], \n",
    " [0.9139333333, 0.7257857143, 0.3062761905], [0.9449571429, 0.7261142857, \n",
    "  0.2886428571], [0.9738952381, 0.7313952381, 0.266647619], \n",
    " [0.9937714286, 0.7454571429, 0.240347619], [0.9990428571, 0.7653142857, \n",
    "  0.2164142857], [0.9955333333, 0.7860571429, 0.196652381], \n",
    " [0.988, 0.8066, 0.1793666667], [0.9788571429, 0.8271428571, 0.1633142857], \n",
    " [0.9697, 0.8481380952, 0.147452381], [0.9625857143, 0.8705142857, 0.1309], \n",
    " [0.9588714286, 0.8949, 0.1132428571], [0.9598238095, 0.9218333333, \n",
    "  0.0948380952], [0.9661, 0.9514428571, 0.0755333333], \n",
    " [0.9763, 0.9831, 0.0538]]\n",
    "\n",
    "parula_map = LinearSegmentedColormap.from_list('parula', cm_data)\n",
    "# For use of \"viscm view\"\n",
    "test_cm = parula_map\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    try:\n",
    "        from viscm import viscm\n",
    "        viscm(parula_map)\n",
    "    except ImportError:\n",
    "        print(\"viscm not found, falling back on simple display\")\n",
    "        plt.imshow(np.linspace(0, 100, 256)[None, :], aspect='auto',\n",
    "                   cmap=parula_map)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.read_excel(join(path_data, 'Phenotypic.xlsx'), sheet_name='surf_n=211', skiprows=0)\n",
    "\n",
    "sub_list = demo['FILE_ID']\n",
    "\n",
    "label = demo['DX_GROUP']\n",
    "site_id = demo['SITE_ID']\n",
    "site_label = demo['SITE_Label']\n",
    "Age = demo['AGE_AT_SCAN']\n",
    "FD = demo['func_mean_fd']\n",
    "\n",
    "ASD_index = np.where(label == 1)[0]                \n",
    "TD_index = np.where(label == 2)[0]\n",
    "Total_index = np.concatenate((ASD_index,TD_index)) \n",
    "sorted_idx = np.concatenate((ASD_index,TD_index), axis = 0)\n",
    "\n",
    "site_label_sorted = np.array(site_label)[sorted_idx]\n",
    "label_sorted = np.array(label)[sorted_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve from each gradient/Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy, TPR, FPR order\n",
    "from sklearn import metrics\n",
    "\n",
    "def make_acc_matfile(roc_matfile, content):\n",
    "    fpr_list=[]\n",
    "    tpr_list=[]\n",
    "    thresh_list=[]\n",
    "    for i in content:\n",
    "        \n",
    "        data = roc_matfile\n",
    "        rm_elm = np.where(np.isnan(data[i]))[1]\n",
    "        data = np.delete(data[i],rm_elm, axis=1)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(data[0], data[1], pos_label=1)\n",
    "        fpr_list.append(fpr)\n",
    "        tpr_list.append(tpr)\n",
    "        thresh_list.append(thresholds)\n",
    "\n",
    "    return fpr_list, tpr_list\n",
    "\n",
    "\n",
    "def make_roc_plot(fpr_list, tpr_list, content):\n",
    "    sns.set_palette('Greys')\n",
    "    fig, ax = plt.subplots(figsize = [8, 7])\n",
    "    for i in range(len(fpr_list)):\n",
    "        label = content[i]\n",
    "\n",
    "        if label == 'nothresh' : # Bold topk revision manually\n",
    "            roc_auc = metrics.auc(fpr_list[i], tpr_list[i])\n",
    "            display = metrics.RocCurveDisplay(fpr=fpr_list[i], tpr=tpr_list[i],  roc_auc=roc_auc)\n",
    "            display.plot(ax=ax, name=label, lw=5, color='k')\n",
    "            plt.xlabel('False Positive Rate', fontsize=30)\n",
    "            plt.ylabel('True Positive Rate', fontsize=30)\n",
    "            plt.xticks(fontsize = 20)\n",
    "            plt.yticks(fontsize = 20)\n",
    "            plt.legend(fontsize = 15, loc='lower right')\n",
    "\n",
    "\n",
    "        else:\n",
    "            roc_auc = metrics.auc(fpr_list[i], tpr_list[i])\n",
    "            display = metrics.RocCurveDisplay(fpr=fpr_list[i], tpr=tpr_list[i],  roc_auc=roc_auc)\n",
    "            display.plot(ax=ax, name = label, lw=1)  \n",
    "            plt.xlabel('False Positive Rate', fontsize=30)\n",
    "            plt.ylabel('True Positive Rate', fontsize=30)\n",
    "            plt.xticks(fontsize = 20)\n",
    "            plt.yticks(fontsize = 20)\n",
    "            plt.legend(fontsize = 15, loc='lower right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = 'Gradient1_pearson_MeanHar_ROCscore'\n",
    "roc_matfile = sc.io.loadmat(join(path_mat, 'review/ROC', f'{mat_file}.mat'))\n",
    "\n",
    "content = ['top01', 'top03', 'top05', 'top10', 'top25', 'top50', 'nothresh']\n",
    "\n",
    "fpr_list, tpr_list = make_acc_matfile(roc_matfile, content)\n",
    "make_roc_plot(fpr_list, tpr_list, content)\n",
    "plt.title('Gradient 1 Train', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = 'Gradient2_pearson_MeanHar_ROCscore'\n",
    "roc_matfile = sc.io.loadmat(join(path_mat, 'review/ROC', f'{mat_file}.mat'))\n",
    "\n",
    "content = ['top01', 'top03', 'top05', 'top10', 'top25', 'top50', 'nothresh']\n",
    "\n",
    "fpr_list, tpr_list = make_acc_matfile(roc_matfile, content)\n",
    "make_roc_plot(fpr_list, tpr_list, content)\n",
    "plt.title('Gradient 2 Train', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = 'Gradient3_pearson_MeanHar_ROCscore'\n",
    "roc_matfile = sc.io.loadmat(join(path_mat, 'review/ROC', f'{mat_file}.mat'))\n",
    "\n",
    "content = ['top01', 'top03', 'top05', 'top10', 'top25', 'top50', 'nothresh']\n",
    "\n",
    "fpr_list, tpr_list = make_acc_matfile(roc_matfile, content)\n",
    "\n",
    "make_roc_plot(fpr_list, tpr_list, content)\n",
    "plt.title('Gradient 3 Train', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = 'Gradient123_pearson_MeanHar_ROCscore'\n",
    "roc_matfile = sc.io.loadmat(join(path_mat, 'review/ROC', f'{mat_file}.mat'))\n",
    "\n",
    "content = ['top01', 'top03', 'top05', 'top10', 'top25', 'top50', 'nothresh']\n",
    "\n",
    "fpr_list, tpr_list = make_acc_matfile(roc_matfile, content)\n",
    "make_roc_plot(fpr_list, tpr_list, content)\n",
    "plt.title('Gradient 123 Train', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = 'Gradient1_pearson_MeanHar_ROCscore_ABD1'\n",
    "roc_matfile = sc.io.loadmat(join(path_mat, 'review/ROC',f'{mat_file}.mat'))\n",
    "\n",
    "content = ['top01', 'top03', 'top05', 'top10', 'top25', 'top50', 'nothresh']\n",
    "\n",
    "fpr_list, tpr_list = make_acc_matfile(roc_matfile, content)\n",
    "make_roc_plot(fpr_list, tpr_list, content)\n",
    "plt.title('Gradient 1 Valid', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = 'Gradient2_pearson_MeanHar_ROCscore_ABD1'\n",
    "roc_matfile = sc.io.loadmat(join(path_mat, 'review/ROC', f'{mat_file}.mat'))\n",
    "\n",
    "content = ['top01', 'top03', 'top05', 'top10', 'top25', 'top50', 'nothresh']\n",
    "\n",
    "fpr_list, tpr_list = make_acc_matfile(roc_matfile, content)\n",
    "make_roc_plot(fpr_list, tpr_list, content)\n",
    "plt.title('Gradient 2 Valid', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = 'Gradient3_pearson_MeanHar_ROCscore_ABD1'\n",
    "roc_matfile = sc.io.loadmat(join(path_mat, 'review/ROC', f'{mat_file}.mat'))\n",
    "\n",
    "content = ['top01', 'top03', 'top05', 'top10', 'top25', 'top50', 'nothresh']\n",
    "\n",
    "fpr_list, tpr_list = make_acc_matfile(roc_matfile, content)\n",
    "make_roc_plot(fpr_list, tpr_list, content)\n",
    "plt.title('Gradient 3 Valid', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = 'Gradient123_pearson_MeanHar_ROCscore_ABD1'\n",
    "roc_matfile = sc.io.loadmat(join(path_mat, 'review/ROC', f'{mat_file}.mat'))\n",
    "\n",
    "content = ['top01', 'top03', 'top05', 'top10', 'top25', 'top50', 'nothresh']\n",
    "\n",
    "fpr_list, tpr_list = make_acc_matfile(roc_matfile, content)\n",
    "make_roc_plot(fpr_list, tpr_list, content)\n",
    "plt.title('Gradient 123 Valid', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding Accuracy Compare (Valid set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, Sensitivity(ASD), Specificity(TD) from FRF result\n",
    "# top01, top03, top05, top10 top25 top50,  noThresh, \n",
    "# grad 1, 2 ,3, 123\n",
    "\n",
    "# Thresholding, gradient order\n",
    "\n",
    "X = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "Y = [0, 1, 2, 3, 4]\n",
    "\n",
    "Top50_ACC_train = [73, 59, 73, 63, None] # grad 1, 2 ,3, 123\n",
    "Top50_Sen_train = [71, 54, 67, 58, None]\n",
    "Top50_Spec_train = [76, 65, 78, 66, None]\n",
    "\n",
    "Top50_ACC_valid = [58, 54, 55, 50, None]\n",
    "Top50_Sen_valid = [57, 53, 49.6, 59, None]\n",
    "Top50_Spec_valid = [58, 55, 61, 42, None]\n",
    "\n",
    "\n",
    "Grad1_ACC_train = [59, 58, 62, 69, 67, 73.4, 59, None, None] # top 01, 03, 05, 10, 25, 50, nothresh\n",
    "Grad2_ACC_train = [61, 68, 59, 68, 69, 59, 64, None, None]\n",
    "Grad3_ACC_train = [64, 66, 73, 76, 75, 72.6, 73, None, None]\n",
    "Grad123_ACC_train = [68, 69, 73, 64, 68, 63, 83, None, None]\n",
    "\n",
    "Grad1_ACC_valid = [44, 54, 51, 51, 46, 58, 53, None, None]\n",
    "Grad2_ACC_valid = [53, 51, 55, 55, 55, 54, 44, None, None]\n",
    "Grad3_ACC_valid = [47, 49, 52, 55, 54, 55, 54, None, None]\n",
    "Grad123_ACC_valid = [51, 53, 54, 52, 54, 50, 55, None, None]\n",
    "\n",
    "sns.set_palette('Greys')\n",
    "\n",
    "# Top K performance\n",
    "plt.figure(1, (20,5)) \n",
    "plt.plot(X,Grad1_ACC_train, ':o', linewidth=1, label = 'Gradient 1', markersize = 20, color = 'black')\n",
    "plt.plot(X,Grad2_ACC_train, ':o', linewidth=1, label = 'Gradient 2', markersize = 20, color = 'darkgray')\n",
    "plt.plot(X,Grad3_ACC_train, ':o', linewidth=1, label = 'Gradient 3', markersize = 20, color = 'silver')\n",
    "plt.plot(X,Grad123_ACC_train, ':o', linewidth=1, label = 'Gradient 123', markersize = 20, color = 'dimgray')\n",
    "\n",
    "plt.ylabel('Accuracy (%)', fontsize = 30)\n",
    "plt.ylim([49, 90])\n",
    "plt.yticks([55, 60, 65, 70, 75, 80, 85], fontsize = 20)\n",
    "plt.xlabel('Threholding', fontsize = 30)\n",
    "plt.xticks(np.arange(9), ('Top 01%', 'Top 03%', 'Top 05%', 'Top 10%', 'Top 25%', 'Top 50%', 'None', '', ''), fontsize = 20)\n",
    "plt.title('Top K FRF perfomance Train', fontsize = 30)\n",
    "plt.legend(loc = 'lower right', fontsize = 20)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figure(2, (20,5)) \n",
    "plt.plot(X,Grad1_ACC_valid, ':o', linewidth=1, label = 'Gradient 1', markersize = 20, color = 'black')\n",
    "plt.plot(X,Grad2_ACC_valid, ':o', linewidth=1, label = 'Gradient 2', markersize = 20, color = 'darkgray')\n",
    "plt.plot(X,Grad3_ACC_valid, ':o', linewidth=1, label = 'Gradient 3', markersize = 20, color = 'silver')\n",
    "plt.plot(X,Grad123_ACC_valid, ':o', linewidth=1, label = 'Gradient 123', markersize = 20, color = 'dimgray')\n",
    "\n",
    "plt.ylabel('Accuracy (%)', fontsize = 30)\n",
    "plt.ylim([37, 64])\n",
    "plt.yticks([40, 45, 50, 55, 60], fontsize = 20)\n",
    "plt.xlabel('Threholding', fontsize = 30)\n",
    "plt.xticks(np.arange(9), ('Top 01%', 'Top 03%', 'Top 05%', 'Top 10%', 'Top 25%', 'Top 50%', 'None', '', ''), fontsize = 20)\n",
    "plt.title('Top K FRF perfomance Valid', fontsize = 30)\n",
    "plt.legend(loc = 'lower right', fontsize = 20)\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# Top50 gradient performance\n",
    "plt.figure(3, (20,5)) \n",
    "plt.plot(Y,Top50_ACC_train, ':o', linewidth=1, label = 'Accuracy', markersize = 20, color = 'black')\n",
    "plt.plot(Y,Top50_Sen_train, ':o', linewidth=1, label = 'Sensitivity (ASD)', markersize = 20, color = 'dimgray')\n",
    "plt.plot(Y,Top50_Spec_train, ':o', linewidth=1, label = 'Specificity (TD)', markersize = 20, color = 'silver')\n",
    "\n",
    "plt.ylabel('Percent (%)', fontsize = 30)\n",
    "plt.ylim([48, 81])\n",
    "plt.yticks([50, 55, 60, 65, 70, 75, 80], fontsize = 20)\n",
    "plt.xlabel('Top 50% gradients', fontsize = 30)\n",
    "plt.xticks(np.arange(5), ('Gradient 1', 'Gradient 2', 'Gradient 3', 'Gradient 123', ''), fontsize = 20)\n",
    "plt.title('Top 50% gradient FRF perfomance Train', fontsize = 30)\n",
    "plt.legend(loc = 'lower right', fontsize = 20)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figure(4, (20,5)) \n",
    "plt.plot(Y,Top50_ACC_valid, ':o', linewidth=1, label = 'Accuracy', markersize = 20, color = 'black')\n",
    "plt.plot(Y,Top50_Sen_valid, ':o', linewidth=1, label = 'Sensitivity (ASD)', markersize = 20, color = 'dimgray')\n",
    "plt.plot(Y,Top50_Spec_valid, ':o', linewidth=1, label = 'Specificity (TD)', markersize = 20, color = 'silver')\n",
    "\n",
    "plt.ylabel('Percent (%)', fontsize = 30)\n",
    "plt.ylim([38, 66])\n",
    "plt.yticks([40, 45, 50, 55, 60, 65], fontsize = 20)\n",
    "plt.xlabel('Top 50% gradients', fontsize = 30)\n",
    "plt.xticks(np.arange(5), ('Gradient 1', 'Gradient 2', 'Gradient 3', 'Gradient 123', ''), fontsize = 20)\n",
    "plt.title('Top 50% gradient FRF perfomance Valid', fontsize = 30)\n",
    "plt.legend(loc = 'lower right', fontsize = 20)\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start characterize subytpe using Funtaional Random Forest result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reg out mat\n",
    "\n",
    "grad_num = 1\n",
    "Thresh = 'top50' # top50\n",
    "ref_Har = 'Mean' # Mean \n",
    "corr = 'pearson' # pearson\n",
    "split_per = 'split06'\n",
    "\n",
    "result_type = 'reviewer' # main_result reviewer simul\n",
    "\n",
    "\n",
    "if result_type == 'main_result':\n",
    "    print('main_result')\n",
    "    regout_mat = sc.io.loadmat(join(path_data,f'Gradient{grad_num}_Surf_{corr}_sorted_regout_{ref_Har}_har_n211_{Thresh}.mat'))\n",
    "\n",
    "    PCs_sorted_reg_out_n211 = regout_mat['total_group_data'][:,:360]\n",
    "    # plt.matshow(DMN_PCs_sorted_reg_out_n211)\n",
    "\n",
    "    grp_ASD_PCs_reg_out = PCs_sorted_reg_out_n211[:103,:]\n",
    "    grp_TD_PCs_reg_out = PCs_sorted_reg_out_n211[103:,:]\n",
    "\n",
    "    # Load FRF subtype result\n",
    "    folder = f'Gradient{grad_num}_{corr}_{ref_Har}Har_{Thresh}_1000iter_1perm_rtoz_FeaSel_output'\n",
    "    FRF_outbputs_DMN_pcs = sc.io.loadmat(join(path_mat,f'{folder}/subgroup_community_assignments.mat'))\n",
    "\n",
    "elif result_type == 'reviewer':\n",
    "    # for reviewer\n",
    "    print('reviewer')\n",
    "    Topk='Top50'\n",
    "    regout_mat = sc.io.loadmat(join(path_data,f'review/{Topk}',f'Gradient{grad_num}_Surf_{corr}_sorted_regout_{ref_Har}_har_n211_{Thresh}_{split_per}.mat')) # for reviewer _2 _3\n",
    "    PCs_sorted_reg_out_n211 = regout_mat['group_data_total'][:,:360] # group_data_total group_data1_total group_data2_total\n",
    "    label = regout_mat['group_data_total'][:,360] # # group_data1_total group_data2_total\n",
    "    grp_ASD_PCs_reg_out = PCs_sorted_reg_out_n211[np.where(label==1)[0]]\n",
    "    grp_TD_PCs_reg_out = PCs_sorted_reg_out_n211[np.where(label==2)[0]]\n",
    "    folder = f'Gradient{grad_num}_{corr}_{ref_Har}Har_{Thresh}_100iter_1perm_rtoz_FeaSel_{split_per}_valid_Main_output' # _output _valid_Main_output _valid_ABD1_output _valid_ABD2_output\n",
    "    FRF_outbputs_DMN_pcs = sc.io.loadmat(join(path_mat,f'review/{Topk}_main',f'{folder}/subgroup_community_assignments.mat'))\n",
    "    \n",
    "    ASD_label_idx = np.where(label==1)[0]\n",
    "    TD_label_idx = np.where(label==2)[0]\n",
    "    \n",
    "elif result_type == 'simul':\n",
    "    print('simul')\n",
    "    Simul_folder = 'Simul_2_35'\n",
    "    regout_mat = sc.io.loadmat(join(path_data,f'review/{Simul_folder}/Gradient_Simul_split06.mat'))\n",
    "    PCs_sorted_reg_out_n211 = regout_mat['group_data_total'][:,:6]\n",
    "    label = regout_mat['group_data_total'][:,6]\n",
    "    grp_ASD_PCs_reg_out = PCs_sorted_reg_out_n211[np.where(label==1)[0]]\n",
    "    grp_TD_PCs_reg_out = PCs_sorted_reg_out_n211[np.where(label==2)[0]]\n",
    "    folder = f'Gradient_Simul_100iter_1perm_{split_per}_valid_Main_output' # _output _valid_Main_output _valid_ABD1_output\n",
    "    FRF_outbputs_DMN_pcs = sc.io.loadmat(join(path_mat,f'review/{Simul_folder}',f'{folder}/subgroup_community_assignments.mat'))\n",
    "\n",
    "# subgroup_order\n",
    "subgroup_order = []\n",
    "\n",
    "for i in range(len(PCs_sorted_reg_out_n211)):\n",
    "    subgroup_order.append(FRF_outbputs_DMN_pcs['subgroup_community_assignments'][i][0][0])\n",
    "    \n",
    "ASD1 = np.where(np.array(subgroup_order)=='G1_1')[0].shape[0]\n",
    "ASD2 = np.where(np.array(subgroup_order)=='G1_2')[0].shape[0]\n",
    "ASD3 = np.where(np.array(subgroup_order)=='G1_3')[0].shape[0]\n",
    "ASD4 = np.where(np.array(subgroup_order)=='G1_4')[0].shape[0]\n",
    "ASD5 = np.where(np.array(subgroup_order)=='G1_5')[0].shape[0]\n",
    "ASD6 = np.where(np.array(subgroup_order)=='G1_6')[0].shape[0]\n",
    "\n",
    "TD1 = np.where(np.array(subgroup_order)=='G2_1')[0].shape[0]\n",
    "TD2 = np.where(np.array(subgroup_order)=='G2_2')[0].shape[0]\n",
    "TD3 = np.where(np.array(subgroup_order)=='G2_3')[0].shape[0]\n",
    "TD4 = np.where(np.array(subgroup_order)=='G2_4')[0].shape[0]\n",
    "TD5 = np.where(np.array(subgroup_order)=='G2_5')[0].shape[0]\n",
    "TD6 = np.where(np.array(subgroup_order)=='G2_6')[0].shape[0]\n",
    "TD7 = np.where(np.array(subgroup_order)=='G2_7')[0].shape[0]\n",
    "\n",
    "# Assign subtype idx\n",
    "ASD_DMN_pc_sub1_idx = np.array(FRF_outbputs_DMN_pcs['subgroup_sorting_orders'][0][0].reshape(-1,),dtype='int64')[ :ASD1]-1\n",
    "ASD_DMN_pc_sub2_idx = np.array(FRF_outbputs_DMN_pcs['subgroup_sorting_orders'][0][0].reshape(-1,),dtype='int64')[ASD1 : ASD1+ASD2]-1\n",
    "ASD_DMN_pc_sub3_idx = np.array(FRF_outbputs_DMN_pcs['subgroup_sorting_orders'][0][0].reshape(-1,),dtype='int64')[ASD1+ASD2 : ASD1+ASD2+ASD3]-1\n",
    "ASD_DMN_pc_sub4_idx = np.array(FRF_outbputs_DMN_pcs['subgroup_sorting_orders'][0][0].reshape(-1,),dtype='int64')[ASD1+ASD2+ASD3 : ASD1+ASD2+ASD3+ASD4]-1   \n",
    "ASD_DMN_pc_sub5_idx = np.array(FRF_outbputs_DMN_pcs['subgroup_sorting_orders'][0][0].reshape(-1,),dtype='int64')[ASD1+ASD2+ASD3+ASD4:ASD1+ASD2+ASD3+ASD4+ASD5]-1   \n",
    "# ASD_DMN_pc_sub6_idx = np.array(FRF_outbputs_DMN_pcs['subgroup_sorting_orders'][0][0].reshape(-1,),dtype='int64')[100:]-1   \n",
    "\n",
    "TD_DMN_pc_sub1_idx = np.array(FRF_outbputs_DMN_pcs['subgroup_sorting_orders'][1][0].reshape(-1,),dtype='int64')[ :TD1]-1\n",
    "TD_DMN_pc_sub2_idx = np.array(FRF_outbputs_DMN_pcs['subgroup_sorting_orders'][1][0].reshape(-1,),dtype='int64')[TD1 : TD1+TD2]-1   \n",
    "TD_DMN_pc_sub3_idx = np.array(FRF_outbputs_DMN_pcs['subgroup_sorting_orders'][1][0].reshape(-1,),dtype='int64')[TD1+TD2 : TD1+TD2+TD3]-1  \n",
    "TD_DMN_pc_sub4_idx = np.array(FRF_outbputs_DMN_pcs['subgroup_sorting_orders'][1][0].reshape(-1,),dtype='int64')[TD1+TD2+TD3 : TD1+TD2+TD3+TD4]-1  \n",
    "TD_DMN_pc_sub5_idx = np.array(FRF_outbputs_DMN_pcs['subgroup_sorting_orders'][1][0].reshape(-1,),dtype='int64')[TD1+TD2+TD3+TD4 :]-1    \n",
    "# TD_DMN_pc_sub6_idx = np.array(FRF_outbputs_DMN_pcs['subgroup_sorting_orders'][1][0].reshape(-1,),dtype='int64')[:]-1    \n",
    "\n",
    "ASD_DMN_pc_sub1 = grp_ASD_PCs_reg_out[ASD_DMN_pc_sub1_idx,:]\n",
    "ASD_DMN_pc_sub2 = grp_ASD_PCs_reg_out[ASD_DMN_pc_sub2_idx,:]\n",
    "ASD_DMN_pc_sub3 = grp_ASD_PCs_reg_out[ASD_DMN_pc_sub3_idx,:]\n",
    "ASD_DMN_pc_sub4 = grp_ASD_PCs_reg_out[ASD_DMN_pc_sub4_idx,:]\n",
    "ASD_DMN_pc_sub5 = grp_ASD_PCs_reg_out[ASD_DMN_pc_sub5_idx,:]\n",
    "\n",
    "TD_DMN_pc_sub1 = grp_TD_PCs_reg_out[TD_DMN_pc_sub1_idx,:]\n",
    "TD_DMN_pc_sub2 = grp_TD_PCs_reg_out[TD_DMN_pc_sub2_idx,:]\n",
    "TD_DMN_pc_sub3 = grp_TD_PCs_reg_out[TD_DMN_pc_sub3_idx,:]\n",
    "TD_DMN_pc_sub4 = grp_TD_PCs_reg_out[TD_DMN_pc_sub4_idx,:]\n",
    "TD_DMN_pc_sub5 = grp_TD_PCs_reg_out[TD_DMN_pc_sub5_idx,:]\n",
    "# TD_DMN_pc_sub6 = grp_TD_PCs_reg_out[TD_DMN_pc_sub6_idx,:]\n",
    "\n",
    "print(ASD_DMN_pc_sub1.shape)\n",
    "print(ASD_DMN_pc_sub2.shape)\n",
    "print(ASD_DMN_pc_sub3.shape)\n",
    "print(ASD_DMN_pc_sub4.shape)\n",
    "print(ASD_DMN_pc_sub5.shape)\n",
    "print(' ')\n",
    "print(TD_DMN_pc_sub1.shape)\n",
    "print(TD_DMN_pc_sub2.shape)\n",
    "print(TD_DMN_pc_sub3.shape)\n",
    "print(TD_DMN_pc_sub4.shape)\n",
    "print(TD_DMN_pc_sub5.shape)\n",
    "# print(TD_DMN_pc_sub6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_06_idx = np.load(join(path_data,'review','Top50_other','Gradient1_split_06_idx.npy'), allow_pickle=True).item()['split_06_idx'] # group_data1_total\n",
    "split_06_idx_rm = np.load(join(path_data,'review','Top50_other','Gradient1_split_06_idx.npy'), allow_pickle=True).item()['split_06_idx_rm'] # group_data2_total, valid ABD1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1876,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADOS_Score assignment\n",
    "\n",
    "label = np.array(demo['DX_GROUP'])\n",
    "\n",
    "ADOS_Total = np.nan_to_num(np.array(demo['ADOS_TOTAL']), nan = -1e-16) \n",
    "ADOS_Total = np.where(ADOS_Total == -9999, -1e-16, ADOS_Total)\n",
    "ADOS_Total\n",
    "ADOS_Total_sorted = ADOS_Total[sorted_idx]\n",
    "\n",
    "ADOS_comm = np.nan_to_num(np.array(demo['ADOS_COMM']), nan = -1e-16)\n",
    "ADOS_comm = np.where(ADOS_comm == -9999, -1e-16, ADOS_comm)\n",
    "ADOS_comm_sorted = ADOS_comm[sorted_idx]\n",
    "\n",
    "ADOS_social =  np.nan_to_num(np.array(demo['ADOS_SOCIAL']), nan = -1e-16)\n",
    "ADOS_social = np.where(ADOS_social == -9999, -1e-16, ADOS_social)\n",
    "ADOS_social_sorted = ADOS_social[sorted_idx]\n",
    "\n",
    "ADOS_behav =  np.nan_to_num(np.array(demo['ADOS_STEREO_BEHAV']), nan = -1e-16)\n",
    "ADOS_behav = np.where(ADOS_behav == -9999, -1e-16, ADOS_behav)\n",
    "ADOS_behav_sorted = ADOS_behav[sorted_idx]\n",
    "\n",
    "SRS = np.nan_to_num(np.array(demo['SRS_RAW_TOTAL']), nan = -2) \n",
    "SRS = np.where(SRS == -9999, -1e-16, SRS)\n",
    "SRS_sorted = SRS[sorted_idx]\n",
    "\n",
    "age_sorted = np.array(Age)[sorted_idx]\n",
    "iq_sorted = np.array(IQ)[sorted_idx]\n",
    "\n",
    "x = ADOS_Total_sorted\n",
    "y = ADOS_comm_sorted\n",
    "z = ADOS_social_sorted\n",
    "t = ADOS_behav_sorted\n",
    "u = SRS_sorted\n",
    "v = age_sorted\n",
    "w = iq_sorted\n",
    "\n",
    "idx1 = ASD_DMN_pc_sub1_idx\n",
    "idx2 = ASD_DMN_pc_sub2_idx\n",
    "idx3 = ASD_DMN_pc_sub3_idx\n",
    "idx4 = ASD_DMN_pc_sub4_idx\n",
    "idx5 = ASD_DMN_pc_sub5_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels as sm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "ADOS_Total = []\n",
    "ADOS_Comm = []\n",
    "ADOS_Social = []\n",
    "ADOS_Behav = []\n",
    "SRS = []\n",
    "age = []\n",
    "iq = []\n",
    "\n",
    "idx_list1 = [idx1, idx1, idx2]\n",
    "idx_list2 = [idx2, idx3, idx3]\n",
    "\n",
    "for i in range(3):\n",
    "    a = idx_list1[i] # idx1 idx1 idx2\n",
    "    b = idx_list2[i] # idx2 idx3 idx3\n",
    "\n",
    "    [s_0,p_0] = sc.stats.ranksums(x[a][x[a]>=0], x[b][x[b]>=0])\n",
    "    [s_1,p_1] = sc.stats.ranksums(y[a][y[a]>=0], y[b][y[b]>=0]) \n",
    "    [s_2,p_2] = sc.stats.ranksums(z[a][z[a]>=0], z[b][z[b]>=0]) \n",
    "    [s_3,p_3] = sc.stats.ranksums(t[a][t[a]>=0], t[b][t[b]>=0]) \n",
    "    [s_4,p_4] = sc.stats.ranksums(u[a][u[a]>=0], u[b][u[b]>=0]) \n",
    "    [s_5,p_5] = sc.stats.ranksums(v[a][v[a]>=0], v[b][v[b]>=0]) \n",
    "    [s_6,p_6] = sc.stats.ranksums(w[a][w[a]>=0], w[b][w[b]>=0]) \n",
    "\n",
    "    p_0_fdr = sm.stats.multitest.multipletests(p_0,alpha=0.05,method='fdr_bh')\n",
    "    p_1_fdr = sm.stats.multitest.multipletests(p_1,alpha=0.05,method='fdr_bh')\n",
    "    p_2_fdr = sm.stats.multitest.multipletests(p_2,alpha=0.05,method='fdr_bh')\n",
    "    p_3_fdr = sm.stats.multitest.multipletests(p_3,alpha=0.05,method='fdr_bh')\n",
    "    p_4_fdr = sm.stats.multitest.multipletests(p_4,alpha=0.05,method='fdr_bh')\n",
    "    p_5_fdr = sm.stats.multitest.multipletests(p_5,alpha=0.05,method='fdr_bh')\n",
    "    p_6_fdr = sm.stats.multitest.multipletests(p_6,alpha=0.05,method='fdr_bh')\n",
    "\n",
    "    ADOS_Total.append(p_0_fdr[1][0])\n",
    "    ADOS_Comm.append(p_1_fdr[1][0])\n",
    "    ADOS_Social.append(p_2_fdr[1][0])\n",
    "    ADOS_Behav.append(p_3_fdr[1][0])\n",
    "    SRS.append(p_4_fdr[1][0])\n",
    "    age.append(p_5_fdr[1][0])\n",
    "    iq.append(p_6_fdr[1][0])\n",
    "\n",
    "result = {'ADOS_Total' : [np.round(i,4) for i in ADOS_Total], \n",
    "          'ADOS_Comm' : [np.round(i,4) for i in ADOS_Comm], \n",
    "          'ADOS_Social' : [np.round(i,4) for i in ADOS_Social],\n",
    "          'ADOS_Behav' : [np.round(i,4) for i in ADOS_Behav], \n",
    "          'SRS' : [np.round(i,4) for i in SRS],\n",
    "          'Age' : [np.round(i,4) for i in age],\n",
    "          'IQ'  : [np.round(i,4) for i in iq]}\n",
    "\n",
    "[f_age,p_age] = sc.stats.f_oneway(np.array(v)[idx1], np.array(v)[idx2], np.array(v)[idx3])\n",
    "[f_iq,p_iq] = sc.stats.f_oneway(np.array(w)[idx1], np.array(w)[idx2], np.array(w)[idx3])\n",
    "\n",
    "for key, val in result.items():\n",
    "    print(f'{key}    :    {val}')    \n",
    "    \n",
    "print(f'Age F : {np.round(f_age,3)}, p : {np.round(p_age,3)}')\n",
    "print(f'IQ F : {np.round(f_iq,3)}, p : {np.round(p_iq,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "FRF_subtype_label = np.zeros(len(ASD_label_idx))\n",
    "FRF_subtype_label[:] = np.nan\n",
    "FRF_subtype_label[idx1] =1\n",
    "FRF_subtype_label[idx2] =2\n",
    "FRF_subtype_label[idx3] =3\n",
    "# FRF_subtype_label[idx4] =4\n",
    "# FRF_subtype_label[idx5] =5\n",
    "FRF_subtype_label\n",
    "\n",
    "# y = ADOS_social_sorted # ADOS_Total_sorted ADOS_comm_sorted ADOS_social_sorted ADOS_behav_sorted SRS_sorted \n",
    "\n",
    "score_name = ['ADOS Total','ADOS Communication','ADOS Social','ADOS Behavior', 'SRS', 'Age', 'IQ']\n",
    "\n",
    "\n",
    "for i, score in enumerate([ADOS_Total_sorted, ADOS_comm_sorted, ADOS_social_sorted, ADOS_behav_sorted, SRS_sorted, age_sorted, iq_sorted]):\n",
    "\n",
    "    df = DataFrame([score[:len(ASD_label_idx)][score[:len(ASD_label_idx)]>=0], FRF_subtype_label[[score[:len(ASD_label_idx)]>=0]]]) # cluster_labels[ADOS_Total_idx_sorted] SRS_exist_idx_sorted\n",
    "    df = df.T\n",
    "    df.columns = [score_name[i],'ASD Subtype']\n",
    "\n",
    "    plt.figure(figsize = (7,7))\n",
    "    sns.set(style = 'whitegrid', font_scale=2.5)\n",
    "    sns.boxplot(x = 'ASD Subtype' , y = df.columns[0], data = df, palette = 'gist_yarg', width=0.5)\n",
    "    sns.swarmplot(x = 'ASD Subtype' , y = df.columns[0], data = df, color = 'k', size = 4)\n",
    "    plt.xticks([0,1,2],['ASD 1','ASD 2', 'ASD 3'], fontsize=40)\n",
    "    plt.xlabel('Subtype', fontsize=40)\n",
    "    plt.ylabel(score_name[i], fontsize=40)\n",
    "#     plt.ylim(0,10)\n",
    "#     plt.yticks([2,4,6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "FRF_subtype_label = np.zeros(103)\n",
    "FRF_subtype_label[:] = np.nan\n",
    "# FRF_subtype_label[idx1] =1\n",
    "# FRF_subtype_label[idx2] =2\n",
    "# FRF_subtype_label[idx3] =3\n",
    "FRF_subtype_label[idx4] =4\n",
    "FRF_subtype_label[idx5] =5\n",
    "FRF_subtype_label\n",
    "\n",
    "# y = ADOS_social_sorted # ADOS_Total_sorted ADOS_comm_sorted ADOS_social_sorted ADOS_behav_sorted SRS_sorted \n",
    "\n",
    "score_name = ['ADOS Total','ADOS Communication','ADOS Social','ADOS Behavior', 'SRS']\n",
    "\n",
    "for i, score in enumerate([ADOS_Total_sorted, ADOS_comm_sorted, ADOS_social_sorted, ADOS_behav_sorted, SRS_sorted]):\n",
    "\n",
    "    df = DataFrame([score[:103][score[:103]>0], FRF_subtype_label[[score[:103]>0]]]) # cluster_labels[ADOS_Total_idx_sorted] SRS_exist_idx_sorted\n",
    "    df = df.T\n",
    "    df.columns = [score_name[i],'ASD Subtype']\n",
    "\n",
    "    plt.figure(figsize = (7,7))\n",
    "    sns.set(style = 'whitegrid', font_scale=2.5)\n",
    "#     sns.boxplot(x = 'ASD Subtype' , y = df.columns[0], data = df, palette = 'gist_yarg', width=0.5)\n",
    "    sns.swarmplot(x = 'ASD Subtype' , y = df.columns[0], data = df, color = 'k', size = 10)\n",
    "    plt.xticks([0],['ASD 4'], fontsize = 40)\n",
    "    plt.xlabel('Subtype', fontsize = 40)\n",
    "    plt.ylabel(score_name[i], fontsize = 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADOS Calibration score\n",
    "\n",
    "demo_cali = pd.read_excel(join(path_data, 'abide_abideI_calibration.xlsx'), sheet_name='n=211', skiprows=0)\n",
    "\n",
    "FileID_cali = demo_cali['ID3']\n",
    "ADOS_Total_cali = demo_cali['CSS_Final']\n",
    "\n",
    "score = []\n",
    "for i in sub_list:\n",
    "    score.append(ADOS_Total_cali[np.where(FileID_cali == i)[0][0]])\n",
    "    \n",
    "score = np.array(score)\n",
    "\n",
    "y = score[ASD_index]\n",
    "\n",
    "df = DataFrame([y[:103][y[:103]>0], FRF_subtype_label[[y[:103]>0]]]) # cluster_labels[ADOS_Total_idx_sorted] SRS_exist_idx_sorted\n",
    "df = df.T\n",
    "df.columns = ['ADOS Total Calibration','ASD Subtype']\n",
    "\n",
    "plt.figure(1,(7,7))\n",
    "sns.set(style = 'whitegrid', font_scale=2.5)\n",
    "sns.boxplot(x = 'ASD Subtype' , y = df.columns[0], data = df, palette = 'gist_yarg', width=0.5)\n",
    "sns.swarmplot(x = 'ASD Subtype' , y = df.columns[0], data = df, color = 'k', size = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ABD1 symptom severity score\n",
    "\n",
    "ABD1_Score = DataFrame([ADOS_Total_sorted[:103], ADOS_comm_sorted[:103], ADOS_social_sorted[:103], ADOS_behav_sorted[:103], SRS_sorted[:103], FRF_subtype_label[:103]]).T\n",
    "ABD1_Score.columns = ['ADOS Total','ADOS Communication','ADOS Social','ADOS Behavior','SRS','ASD Subtype']\n",
    "ABD1_Score\n",
    "# ABD1_Score.to_excel(join(path_data,'ABD1_Score_revision.xlsx'), sheet_name = 'Sheet1', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1605,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ttest\n",
    "def subtype_ranksum(a,b, whole_region=False, print_show=True):\n",
    "    import statsmodels as sm\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    \n",
    "    input_a = a # [X, Y, Z, X, X, Y, W]\n",
    "    input_b = b # [S, S, S, Y, Z, Z, R]\n",
    "\n",
    "    s_1 = []\n",
    "    p_1 = []\n",
    "\n",
    "    for col in range(input_a.shape[1]):\n",
    "        [s,p] = sc.stats.ranksums(input_a[:,col],input_b[:,col])\n",
    "        s_1.append(s)\n",
    "        p_1.append(p)\n",
    "\n",
    "    s_1 = np.array(s_1)\n",
    "    p_1 = np.array(p_1)\n",
    "\n",
    "    # Calculate Effect size r\n",
    "    effect_r = s_1/np.sqrt(len(input_a)+len(input_b))\n",
    "\n",
    "    p_1_fdr_05 = sm.stats.multitest.multipletests(p_1,alpha=0.05,method='fdr_bh')\n",
    "    p_1_fdr_01 = sm.stats.multitest.multipletests(p_1,alpha=0.01,method='fdr_bh')\n",
    "    p_1_fdr_001 = sm.stats.multitest.multipletests(p_1,alpha=0.001,method='fdr_bh')\n",
    "    \n",
    "    if print_show:\n",
    "        print('FDR uncorrected : ', np.where(p_1 <0.05)[0], np.where(p_1 <0.05)[0].shape )\n",
    "        print('')\n",
    "        print(np.where(p_1_fdr_05[0]==True),'\\n', np.where(p_1_fdr_05[0]==True)[0].shape)\n",
    "        print('FDR 0.05 : ', p_1_fdr_05[1][np.where(p_1_fdr_05[0]==True)[0]])\n",
    "        print('')\n",
    "    \n",
    "    # print('FDR 0.05 : ', p_1_fdr_05[1][np.where(p_1_fdr_05[0]==True)[0]])\n",
    "#     print('')\n",
    "#     print(np.where(p_1_fdr_01[0]==True),'\\n', np.where(p_1_fdr_01[0]==True)[0].shape)\n",
    "#     # print('FDR 0.01 : ', p_1_fdr_01[1][np.where(p_1_fdr_01[0]==True)[0]])\n",
    "#     print('')\n",
    "#     print(np.where(p_1_fdr_001[0]==True),'\\n', np.where(p_1_fdr_001[0]==True)[0].shape)\n",
    "#     # print('FDR 0.001 : ', p_1_fdr_001[1][np.where(p_1_fdr_001[0]==True)[0]])\n",
    "\n",
    "    sign_idx = np.where(p_1_fdr_05[0]==True)[0] # p_1_fdr_05[0]==True p_1<0.05\n",
    "    sign_idx_uncorr = np.where(p_1 <0.05)[0]\n",
    "\n",
    "    ttest_tval = np.zeros(360)\n",
    "\n",
    "    input_idx = sign_idx \n",
    "\n",
    "    ttest_tval[input_idx] = effect_r[input_idx] \n",
    "\n",
    "    if whole_region:\n",
    "        return effect_r\n",
    "    else:\n",
    "        return ttest_tval, sign_idx, p_1_fdr_05[1]\n",
    "\n",
    "# Anova\n",
    "def subtype_kruskal(a,b,c, whole_region=False, print_show=True):\n",
    "    import statsmodels as sm\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    \n",
    "    input_a = a\n",
    "    input_b = b\n",
    "    input_c = c\n",
    "\n",
    "    f_1 = []\n",
    "    p_1 = []\n",
    "    for col in range(input_a.shape[1]):\n",
    "        [f,p] = sc.stats.kruskal(input_a[:,col], input_b[:,col], input_c[:,col])\n",
    "        f_1.append(f)\n",
    "        p_1.append(p)\n",
    "\n",
    "    f_1 = np.array(f_1)\n",
    "    p_1 = np.array(p_1)\n",
    "\n",
    "    # Caculate effect size eps_squared\n",
    "    grp_num = len([input_a, input_b, input_c])\n",
    "    data_num = input_a.shape[0]+input_b.shape[0]+input_c.shape[0]\n",
    "    eps_squared = (f_1-grp_num+1)/(data_num-grp_num)\n",
    "    \n",
    "    if print_show:\n",
    "        print('Subject num : ', input_a.shape[0], input_b.shape[0], input_c.shape[0])\n",
    "\n",
    "    p_1_fdr = sm.stats.multitest.multipletests(p_1,alpha=0.05,method='fdr_bh')\n",
    "    \n",
    "    if print_show:\n",
    "        print(np.where(p_1_fdr[0]==True),'\\n', np.where(p_1_fdr[0]==True)[0].shape)\n",
    "\n",
    "    sign_idx = np.where(p_1_fdr[0]==True)[0] # p_1_fdr[0]==True p_1 < 0.05\n",
    "    sign_idx_uncorr = np.where(p_1 <0.05)[0]\n",
    "\n",
    "    ANOVA_fval = np.zeros(360)\n",
    "#     ANOVA_fval[:] = 0.5\n",
    "\n",
    "    input_idx =  sign_idx\n",
    "\n",
    "    ANOVA_fval[input_idx] = eps_squared[input_idx] # cohens_f f_1 eps_squared\n",
    "    \n",
    "    if whole_region:\n",
    "        return eps_squared\n",
    "    else:\n",
    "        return ANOVA_fval, sign_idx, p_1_fdr[1]\n",
    "\n",
    "# Prepare visualization\n",
    "def ROI_visualization(input_stat,stats = 'Ttest'):\n",
    "\n",
    "    import vtk\n",
    "\n",
    "    from vtk import vtkPolyDataNormals\n",
    "\n",
    "    from brainspace.mesh.mesh_io import read_surface\n",
    "    from brainspace.mesh.mesh_operations import combine_surfaces\n",
    "    from brainspace.utils.parcellation import reduce_by_labels\n",
    "    from brainspace.vtk_interface import wrap_vtk, serial_connect\n",
    "\n",
    "    template_path = join(atlas_path, \"MMP\")\n",
    "    template_L = \"L.very_inflated_MSMAll.10k_fs_LR.surf.gii\" # S900.L.midthickness_MSMAll.10k_fs_LR.surf.gii # L.very_inflated_MSMAll.10k_fs_LR.surf.gii\n",
    "    template_R = \"R.very_inflated_MSMAll.10k_fs_LR.surf.gii\" # S900.R.midthickness_MSMAll.10k_fs_LR.surf.gii # R.very_inflated_MSMAll.10k_fs_LR.surf.gii\n",
    "\n",
    "    surfs = [None] * 2\n",
    "    \n",
    "    surfs[0] = read_surface(join(template_path,template_L)) # Z:/hschoi/backup/hschoi/template/MMP/S900.L.midthickness_MSMAll.10k_fs_LR.surf.gii\n",
    "    nf = wrap_vtk(vtkPolyDataNormals, splitting=False, featureAngle=0.1)\n",
    "    surf_lh = serial_connect(surfs[0], nf)\n",
    "\n",
    "    surfs[1] = read_surface(join(template_path,template_R)) # Z:/hschoi/backup/hschoi/template/MMP/S900.R.midthickness_MSMAll.10k_fs_LR.surf.gii\n",
    "    nf = wrap_vtk(vtkPolyDataNormals, splitting=False, featureAngle=0.1)\n",
    "    surf_rh = serial_connect(surfs[1], nf)\n",
    "\n",
    "    # Visualization\n",
    "\n",
    "    from brainspace.datasets import load_group_fc, load_parcellation, load_conte69\n",
    "    from brainspace.gradient import GradientMaps\n",
    "    from brainspace.plotting import plot_hemispheres\n",
    "    from brainspace.utils.parcellation import map_to_labels\n",
    "\n",
    "    atlas = np.load(join(template_path, \"MMP.10k_fs_LR.npy\"))\n",
    "    labeling = atlas\n",
    "    conn_matrix = input_stat \n",
    "    mask = labeling != 0\n",
    "\n",
    "    grad = map_to_labels(conn_matrix, labeling, mask=mask, fill=np.nan) # fill = np.nan fill = 0\n",
    "\n",
    "    if stats == 'ANOVA':\n",
    "        plot_hemispheres(surf_lh, surf_rh, array_name=grad, size=(1300, 300),\n",
    "                         color_bar=True, cmap='Reds', zoom=1.25, nan_color=(0,0,0,1) ,color_range = (0,0.3) , view = 'dorsal'  ) \n",
    "                                                                                                                         \n",
    "    elif stats == 'Ttest':\n",
    "        plot_hemispheres(surf_lh, surf_rh, array_name=grad, size=(1300, 300),\n",
    "                         color_bar=True, cmap='seismic', zoom=1.25, nan_color=(0,0,0,1) ,color_range = (-1,1)   ) # ventral dorsal\n",
    "        \n",
    "    elif stats == 'Hetero':\n",
    "        plot_hemispheres(surf_lh, surf_rh, array_name=grad, size=(1300, 300),\n",
    "                         color_bar=True, cmap='Blues', zoom=1.25, nan_color=(0,0,0,1) ,color_range = (0,0.2)   )\n",
    "                                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttest\n",
    "X = ASD_DMN_pc_sub1 \n",
    "Y = ASD_DMN_pc_sub2\n",
    "Z = ASD_DMN_pc_sub3\n",
    "W = TD_DMN_pc_sub1\n",
    "R = TD_DMN_pc_sub2 \n",
    "V = TD_DMN_pc_sub3\n",
    "\n",
    "S = np.concatenate((W,R,V))\n",
    "U = np.concatenate((X,Y,Z))\n",
    "\n",
    "print('T-test')\n",
    "print('sub1')\n",
    "effect_r_ASD1_TD, ASD1_TD_idx, ASD1_TD_p = subtype_ranksum(X,S, whole_region=False, print_show=True)\n",
    "print('sub2')\n",
    "effect_r_ASD2_TD, ASD2_TD_idx, ASD2_TD_p = subtype_ranksum(Y,S, whole_region=False, print_show=True)\n",
    "print('sub3')\n",
    "effect_r_ASD3_TD, ASD3_TD_idx, ASD3_TD_p = subtype_ranksum(Z,S, whole_region=False, print_show=True)\n",
    "print('sub12')\n",
    "effect_r_ASD12, ASD12_idx, ASD12_p = subtype_ranksum(X,Y, whole_region=False, print_show=True)\n",
    "print('sub13')\n",
    "effect_r_ASD13, ASD13_idx, ASD13_p = subtype_ranksum(X,Z, whole_region=False, print_show=True)\n",
    "print('sub23')\n",
    "effect_r_ASD23, ASD23_idx, ASD23_p = subtype_ranksum(Y,Z, whole_region=False, print_show=True)\n",
    "print('TD12')\n",
    "effect_r_TD12, TD12_idx, TD12_p = subtype_ranksum(W,R, whole_region=False, print_show=True)\n",
    "print('TD13')\n",
    "effect_r_TD13, TD13_idx, TD13_p = subtype_ranksum(W,V)\n",
    "print('TD23')\n",
    "effect_r_TD23, TD23_idx, TD23_p = subtype_ranksum(R,V)\n",
    "\n",
    "print('')\n",
    "print('ANOVA')\n",
    "ASD123_eta, ASD123_idx, ASD123_p = subtype_kruskal(X,Y,Z, whole_region=False, print_show=True)\n",
    "TD123_eta, TD123_idx, TD123_p = subtype_kruskal(W,R,V, whole_region=False, print_show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [],
   "source": [
    "MMP_label = np.load(join(atlas_path,'MMP','MMP_label.npy'))\n",
    "# MMP_label\n",
    "\n",
    "def MMP_section(idx, effect_r, effect_r_p):\n",
    "\n",
    "    for i in range(MMP_label.max()+1):\n",
    "\n",
    "        coregion = np.array(sorted(set(np.where(MMP_label==i)[0]) & set(idx)))\n",
    "\n",
    "        if len(coregion)==0:\n",
    "            L_coregion = 0\n",
    "            R_coregion = 0\n",
    "            L_effect_r_corregion = 0\n",
    "            R_effect_r_corregion = 0\n",
    "            L_p = 1\n",
    "            R_p = 1\n",
    "        else:\n",
    "            L_coregion = coregion[coregion<180]\n",
    "            R_coregion = coregion[coregion>=180]\n",
    "\n",
    "            L_effect_r_corregion = np.round(effect_r[L_coregion].mean(),2)\n",
    "            R_effect_r_corregion = np.round(effect_r[R_coregion].mean(),2)\n",
    "\n",
    "            L_p = effect_r_p[L_coregion]\n",
    "            R_p = effect_r_p[R_coregion]\n",
    "\n",
    "        print(f'Region {i}\\n', f'L : {L_coregion}, R : {R_coregion}', f'{len(coregion)}개\\n', f'L : {L_effect_r_corregion}, R : {R_effect_r_corregion}', '\\n', f'L : {np.round(L_p,4)}, R : {np.round(R_p,4)}')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMP_section(ASD2_TD_idx, effect_r_ASD2_TD, ASD2_TD_p)\n",
    "MMP_section(ASD12_idx, effect_r_ASD12, ASD12_p)\n",
    "# MMP_section(TD123_idx, TD123_eta, TD123_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI Visulaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI_visualization(TD123_eta, stats = 'ANOVA')\n",
    "ROI_visualization(effect_r_TD23, stats = 'Ttest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make file for Neurosynth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_for_Neurosynth(input_stats, effect_r, subtype_name = 'ASD1'):\n",
    "\n",
    "    template_path = join(atlas_path, \"MMP\")\n",
    "    atlas = np.load(join(template_path, \"MMP.10k_fs_LR.npy\"))\n",
    "\n",
    "    atlas_L = atlas[:10242]\n",
    "    atlas_R = atlas[10242:]\n",
    "\n",
    "    # atlas.max()\n",
    "\n",
    "    atlas_sig=np.zeros(len(atlas))\n",
    "    for i in input_stats:                       # ASD12_idx ASD13_idx ASD23_idx ASD1_TD12_idx ASD2_TD12_idx ASD3_TD12_idx\n",
    "        print(i+1, ' ', end='', flush=True)\n",
    "\n",
    "        atlas_sig[np.where(atlas==i+1)[0]] = effect_r[i]\n",
    "\n",
    "    atlas_sig_L = atlas_sig[:10242]\n",
    "    atlas_sig_R = atlas_sig[10242:]\n",
    "\n",
    "    np.save(join(path_gii_data, 'revision', f'L.MMP.10k.ef_r_{subtype_name}_TD.npy'),atlas_sig_L) # atlas\n",
    "    np.save(join(path_gii_data, 'revision', f'R.MMP.10k.ef_r_{subtype_name}_TD.npy'),atlas_sig_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199  "
     ]
    }
   ],
   "source": [
    "file_for_Neurosynth(ASD1_TD_idx, effect_r_ASD1_TD, subtype_name = 'ASD1')\n",
    "file_for_Neurosynth(ASD2_TD_idx, effect_r_ASD2_TD, subtype_name = 'ASD2')\n",
    "file_for_Neurosynth(ASD3_TD_idx, effect_r_ASD3_TD, subtype_name = 'ASD3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASD/TD heterogeneity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1866,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = f'noaff_surf_mat_Mean_har_n211_gradients_top50.npy'\n",
    "gradients = np.load(join(path_work,data))\n",
    "gradients_1 = gradients[:,:,0]\n",
    "\n",
    "ASD_gradient = gradients_1[:103,:]\n",
    "TD_gradient = gradients_1[103:,:]\n",
    "\n",
    "ASD_gradient_sub1 = ASD_gradient[ASD_DMN_pc_sub1_idx,:]\n",
    "ASD_gradient_sub2 = ASD_gradient[ASD_DMN_pc_sub2_idx,:]\n",
    "ASD_gradient_sub3 = ASD_gradient[ASD_DMN_pc_sub3_idx,:]\n",
    "\n",
    "TD_gradient_sub1 = TD_gradient[TD_DMN_pc_sub1_idx,:]\n",
    "TD_gradient_sub2 = TD_gradient[TD_DMN_pc_sub2_idx,:]\n",
    "TD_gradient_sub3 = TD_gradient[TD_DMN_pc_sub3_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "X = ASD_gradient_sub1 #ASD_DMN_pc_sub1 ASD_gradient_sub1\n",
    "Y = ASD_gradient_sub2 #ASD_DMN_pc_sub2 ASD_gradient_sub2\n",
    "Z = ASD_gradient_sub3 #ASD_DMN_pc_sub3 ASD_gradient_sub3\n",
    "W = TD_gradient_sub1 #TD_DMN_pc_sub1 TD_gradient_sub1\n",
    "R = TD_gradient_sub2 #TD_DMN_pc_sub2 TD_gradient_sub2\n",
    "V = TD_gradient_sub3 #TD_DMN_pc_sub3 TD_gradient_sub3\n",
    "\n",
    "# ASD_eta_list=[]\n",
    "# TD_eta_list=[]\n",
    "\n",
    "ASD_mean_list=[]\n",
    "ASD_std_list=[]\n",
    "TD_mean_list=[]\n",
    "TD_std_list=[]\n",
    "\n",
    "for i in range(100):\n",
    "    print(i, ' ', end='', flush=True)\n",
    "\n",
    "    X_boot = np.array(random.sample(X.tolist(),int(len(X)*0.9)))\n",
    "    Y_boot = np.array(random.sample(Y.tolist(),int(len(Y)*0.9)))\n",
    "    Z_boot = np.array(random.sample(Z.tolist(),int(len(Z)*0.9)))\n",
    "    W_boot = np.array(random.sample(W.tolist(),int(len(W)*0.9)))\n",
    "    R_boot = np.array(random.sample(R.tolist(),int(len(R)*0.9)))\n",
    "    V_boot = np.array(random.sample(V.tolist(),int(len(V)*0.9)))\n",
    "\n",
    "    ASD_eta = subtype_kruskal(X_boot,Y_boot,Z_boot, whole_region=True, print_show=False)\n",
    "    TD_eta = subtype_kruskal(W_boot,R_boot,V_boot, whole_region=True, print_show=False)\n",
    "    \n",
    "    ASD_mean = np.concatenate((X_boot, Y_boot, Z_boot), axis=0).mean(axis=0)\n",
    "    ASD_std = np.concatenate((X_boot, Y_boot, Z_boot), axis=0).std(axis=0)\n",
    "    TD_mean = np.concatenate((W_boot,R_boot,V_boot), axis=0).mean(axis=0)\n",
    "    TD_std = np.concatenate((W_boot,R_boot,V_boot), axis=0).std(axis=0)\n",
    "    \n",
    "    \n",
    "    ASD_eta_list.append(ASD_eta)\n",
    "    TD_eta_list.append(TD_eta)\n",
    "    \n",
    "    ASD_mean_list.append(ASD_mean)\n",
    "    ASD_std_list.append(ASD_std)\n",
    "    TD_mean_list.append(TD_mean)\n",
    "    TD_std_list.append(TD_std)\n",
    "\n",
    "ASD_eta_mean = np.stack(ASD_eta_list).mean(axis=0)\n",
    "ASD_eta_std = np.stack(ASD_eta_list).std(axis=0)\n",
    "TD_eta_mean = np.stack(TD_eta_list).mean(axis=0)\n",
    "TD_eta_std = np.stack(TD_eta_list).std(axis=0)\n",
    "\n",
    "ASD_grad_mean = np.stack(ASD_mean_list).mean(axis=0)\n",
    "ASD_var_mean = np.stack(ASD_std_list).mean(axis=0)\n",
    "TD_grad_mean = np.stack(TD_mean_list).mean(axis=0)\n",
    "TD_var_mean = np.stack(TD_std_list).mean(axis=0)\n",
    "\n",
    "ASD_var_main = np.concatenate((X, Y, Z), axis=0).std(axis=0)\n",
    "TD_var_main = np.concatenate((W,R,V), axis=0).std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASD/TD variability (gradient std) comparison\n",
    "\n",
    "ASDTD_idx = np.array(sorted(set(ASD123_idx) | set(TD123_idx)))\n",
    "\n",
    "input_asd = ASD_var_main # ASD_var_main ASD_var_mean\n",
    "input_td = TD_var_main # TD_var_main TD_var_mean\n",
    "\n",
    "\n",
    "df_asd=pd.DataFrame(np.concatenate((input_asd.reshape(-1,1), np.zeros(len(input_asd)).reshape(-1,1)),axis=1), columns=['var','Label'])\n",
    "df_td=pd.DataFrame(np.concatenate((input_td.reshape(-1,1), np.ones(len(input_td)).reshape(-1,1)),axis=1), columns=['var','Label'])\n",
    "df_hetero = pd.concat([df_asd, df_td], ignore_index=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (7,7))\n",
    "sns.set(style = 'whitegrid', font_scale=2.5)\n",
    "sns.boxplot(x = 'Label' , y = 'var', data = df_hetero, palette = 'gist_yarg', width=0.5)\n",
    "sns.swarmplot(x = 'Label' , y = 'var', data = df_hetero, color = 'k', size = 4)\n",
    "plt.xlabel('Group')\n",
    "plt.ylabel('Variability')\n",
    "plt.xticks([0,1],['ASD','TD'])\n",
    "# plt.ylim([-0.03,0.3])\n",
    "\n",
    "\n",
    "t_hetero, p_hetero = sc.stats.ranksums(input_asd, input_td) # sc.stats.ranksums sc.stats.ttest_ind\n",
    "print('t stattic : ', np.round(t_hetero,2))\n",
    "print('p value : ', p_hetero)\n",
    "f'ASD is about {np.round((input_asd/input_td).mean()*100-100,2)}% larger'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.read_excel(join(path_data, 'Phenotypic.xlsx'), sheet_name='surf_n=211', skiprows=0)\n",
    "\n",
    "sub_list = demo['FILE_ID']\n",
    "label = demo['DX_GROUP']\n",
    "Age = demo['AGE_AT_SCAN']\n",
    "FD = demo['func_mean_fd']\n",
    "\n",
    "\n",
    "ASD_index = np.where(label == 1)[0]                \n",
    "TD_index = np.where(label == 2)[0]\n",
    "Total_index = np.concatenate((ASD_index,TD_index)) \n",
    "sorted_idx = np.concatenate((ASD_index,TD_index), axis = 0)\n",
    "\n",
    "file_list = np.array([glob(join(path_data,f'{workfolder}',f'{sub_list[i]}','*surf_conn_mat_Mean_har.npy')) for i in range(len(sub_list))]).reshape(-1)\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure, sym_matrix_to_vec, vec_to_sym_matrix\n",
    "import statsmodels as sm\n",
    "from statsmodels.api import GLM\n",
    "\n",
    "list_aff = [np.load(sl) for sl in file_list]\n",
    "n = len(list_aff)\n",
    "list_aff_vec = [None] * n\n",
    "\n",
    "for i, x1 in enumerate(list_aff):\n",
    "    print(i,' ', end = '', flush = True)\n",
    "    x1 = x1 + np.eye(len(x1))\n",
    "    list_aff_vec[i] = sym_matrix_to_vec(x1, discard_diagonal=True)\n",
    "\n",
    "# FC age/FD regression out (for DC)\n",
    "list_reg_out = []\n",
    "\n",
    "for i in range(np.array(list_aff_vec).shape[1]):\n",
    "    if i%1000 == 0:\n",
    "        print(i, ' ', end='', flush=True)\n",
    "    a = GLM(np.array(list_aff_vec)[:,i],sm.api.add_constant(np.array([Age,FD,Age*FD]).T))\n",
    "    res = a.fit()\n",
    "    list_reg_out.append(res.resid_response)\n",
    "    \n",
    "FC_reg_out = np.array(list_reg_out).T\n",
    "\n",
    "grp_ASD_FC_reg_out = FC_reg_out[ASD_index,:]\n",
    "grp_TD_FC_reg_out = FC_reg_out[TD_index,:]\n",
    "print('Finish')\n",
    "\n",
    "surf_conn_mat_reg_out_har = [None] * n\n",
    "\n",
    "for i, x1 in enumerate(FC_reg_out):\n",
    "    print(i,' ', end = '', flush = True)\n",
    "    \n",
    "    x2 = vec_to_sym_matrix(x1, diagonal = np.zeros(360))\n",
    "    surf_conn_mat_reg_out_har[i] = x2\n",
    "\n",
    "for i, filename in enumerate(file_list):\n",
    "    print(i)\n",
    "    FileID_num = np.array(sub_list)[i]\n",
    "    save_path = glob(join(path_data,f'{workfolder}',f'*{FileID_num}'))\n",
    "    print(FileID_num, ' ', save_path)\n",
    "      \n",
    "#     np.save(join(save_path[0], 'surf_conn_mat_Age_out_Mean_har.npy'), np.array(surf_conn_mat_reg_out_har[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# soft thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Individual mat\n",
    "\n",
    "file_list = np.array([glob(join(path_data,f'{workfolder}',f'{sub_list[i]}','noaff_surf_mat_reg_out_Mean_har_top50.npy')) for i in range(len(sub_list))]).reshape(-1)\n",
    "\n",
    "path_work = join(path_data,f'{workfolder}')\n",
    "\n",
    "for i, x in enumerate(file_list):\n",
    "    print(i, ' ', end = '' , flush = True)\n",
    "    conn_mat = np.load(x)\n",
    "    z_conn_mat = np.arctanh(np.nan_to_num(conn_mat, nan=0.0))\n",
    "    noaff_conn_mat = gradient.compute_affinity(np.nan_to_num(z_conn_mat, nan=0.0), sparsity=0.5)\n",
    "    np.save(join(path_work, x.split('\\\\')[-2],'noaff_surf_mat_reg_out_Mean_har_top50.npy'),noaff_conn_mat)\n",
    "\n",
    "beta = 6\n",
    "for i, x1 in enumerate(file_list):\n",
    "    print(i, ' ', end = '' , flush = True)\n",
    "    FileID_num = np.array(FileID)[i]\n",
    "    save_path = glob(join(path_data,f'{workfolder}',f'*{FileID_num}'))\n",
    "    \n",
    "    conn_mat = np.load(x1)\n",
    "    conn_mat = np.tanh(conn_mat) # Thresholding mat is rtoz already\n",
    "    soft_thresh = np.power(((conn_mat+1)/2),beta)\n",
    "    z_soft_thresh = np.arctanh(np.nan_to_num(soft_thresh, nan=0.0))\n",
    "    degree = z_soft_thresh.sum(axis=0)\n",
    "    \n",
    "    print(FileID_num, ' ', save_path)\n",
    "    np.save(join(save_path[0], 'surf_degree_pearson_reg_out_Mean_har_top50.npy'), degree)\n",
    "\n",
    "DC=[]\n",
    "\n",
    "for i, x1 in enumerate(file_list):\n",
    "    print(i, ' ', end = '' , flush = True)\n",
    "    x2 = np.load(x1)\n",
    "    DC.append(x2)\n",
    "    \n",
    "# np.save(\"Z:/hschoi/backup/hschoi/1.asd.grad/data/Degree_Surf_pearson_sorted_reg_out_Mean_har_n211_top50\",np.array(DC)[sorted_idx,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate delta gradient and DC between subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_delta(left,right, name = 'degree1_TD'):\n",
    "\n",
    "    left_data = left # grad_sub1 grad_sub2 grad_sub3 degree_sub1 degree_sub2 degree_sub3\n",
    "    right_data = right\n",
    "    save_name = name\n",
    "\n",
    "    list_delta = []\n",
    "\n",
    "    for i, x in enumerate(left_data):\n",
    "        print(i)\n",
    "        x = left_data[i,:]\n",
    "        for j, y in enumerate(right_data):\n",
    "            print(j, ' ', end = '' , flush = True)\n",
    "            y = right_data[j,:]\n",
    "            delta = x-y\n",
    "            list_delta.append(delta)\n",
    "\n",
    "        print('\\n')\n",
    "    np.save(join(path_data, 'review/Top50_main/DC',f'Delta_{save_name}_Surf_pearson_sorted_reg_out_Mean_har_n211_top50'), list_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient1 = np.load(join(path_data, 'Gradient1_Surf_pearson_sorted_regout_Mean_har_n211_top50.npy'))\n",
    "degree = np.load(join(path_data, 'Degree_Surf_pearson_sorted_regout_Mean_har_n211_top50.npy')) \n",
    "\n",
    "gradient1_ASD = gradient1[np.where(label_sorted==1)[0],:]\n",
    "gradient1_TD = gradient1[np.where(label_sorted==2)[0],:]\n",
    "degree_ASD = degree[np.where(label_sorted==1)[0],:]\n",
    "degree_TD = degree[np.where(label_sorted==2)[0],:]\n",
    "\n",
    "TD_sub123_idx = np.concatenate((TD_DMN_pc_sub1_idx,TD_DMN_pc_sub2_idx,TD_DMN_pc_sub3_idx))\n",
    "\n",
    "\n",
    "grad_sub1 = gradient1_ASD[ASD_DMN_pc_sub1_idx,:]\n",
    "grad_sub2 = gradient1_ASD[ASD_DMN_pc_sub2_idx,:]\n",
    "grad_sub3 = gradient1_ASD[ASD_DMN_pc_sub3_idx,:]\n",
    "grad_TD_sub123 = gradient1_TD[TD_sub123_idx, :]\n",
    "\n",
    "degree_sub1 = degree_ASD[ASD_DMN_pc_sub1_idx,:]\n",
    "degree_sub2 = degree_ASD[ASD_DMN_pc_sub2_idx,:]\n",
    "degree_sub3 = degree_ASD[ASD_DMN_pc_sub3_idx,:]\n",
    "degree_TD_sub123 = degree_TD[TD_sub123_idx, :]\n",
    "\n",
    "cal_delta(grad_sub1, grad_sub2, name = 'grad12')\n",
    "cal_delta(grad_sub1, grad_sub3, name = 'grad13')\n",
    "cal_delta(grad_sub2, grad_sub3, name = 'grad23')\n",
    "\n",
    "cal_delta(grad_sub1, grad_TD_sub123, name = 'grad1_TD')\n",
    "cal_delta(grad_sub2, grad_TD_sub123, name = 'grad2_TD')\n",
    "cal_delta(grad_sub3, grad_TD_sub123, name = 'grad3_TD')\n",
    "\n",
    "cal_delta(degree_sub1, degree_sub2, name = 'degree12')\n",
    "cal_delta(degree_sub1, degree_sub3, name = 'degree13')\n",
    "cal_delta(degree_sub2, degree_sub3, name = 'degree23')\n",
    "\n",
    "cal_delta(degree_sub1, degree_TD_sub123, name = 'degree1_TD')\n",
    "cal_delta(degree_sub2, degree_TD_sub123, name = 'degree2_TD')\n",
    "cal_delta(degree_sub3, degree_TD_sub123, name = 'degree3_TD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between delata gradient and delta degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1882,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_DC_corr(input_grad,input_degree):\n",
    "\n",
    "    input_grad = input_grad\n",
    "    input_degree = input_degree\n",
    "    shape = input_grad.shape[1]\n",
    "    title = 'a'# x3_cortex_title\n",
    "\n",
    "    list_r = []\n",
    "    list_p = []\n",
    "\n",
    "    for i in range(shape):\n",
    "        print(i, ' ', end = '', flush = True)\n",
    "        [r,p] = sc.stats.pearsonr(input_grad[:,i],input_degree[:,i])\n",
    "        list_r.append(r)\n",
    "        list_p.append(p)\n",
    "\n",
    "    list_p_fdr = sm.stats.multitest.multipletests(list_p,alpha=0.05,method='fdr_bh')\n",
    "    print(np.where(list_p_fdr[0]==True),'\\n', np.where(list_p_fdr[0]==True)[0].shape)\n",
    "\n",
    "    for n in [0,1]:\n",
    "\n",
    "        plt.figure(figsize=(10,7))\n",
    "        plt.xlim(input_grad[:,n].min()-0.1,input_grad[:,n].max()+0.1)\n",
    "\n",
    "        sns.regplot(input_grad[:,n],input_degree[:,n], scatter_kws={\"color\": 'slategray'}, line_kws={\"color\":'k'})\n",
    "\n",
    "        plt.xlabel('   Gradient', fontsize=60)\n",
    "        plt.ylabel('   DC', fontsize=60)\n",
    "        plt.title(f'{n+1}.  r = {round(list_r[n],3)}  FDR < {np.where(list_p[n]<0.001,0.001,round(list_p[n],2))}', fontsize=50)\n",
    "        plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# significnat region\n",
    "\n",
    "delta_grad12 = np.load(join(path_data, 'review/Top50_main/DC', 'Delta_grad12_Surf_pearson_sorted_reg_out_Mean_har_n211_top50.npy'))\n",
    "delta_grad13 = np.load(join(path_data, 'review/Top50_main/DC', 'Delta_grad13_Surf_pearson_sorted_reg_out_Mean_har_n211_top50.npy'))\n",
    "delta_grad23 = np.load(join(path_data, 'review/Top50_main/DC', 'Delta_grad23_Surf_pearson_sorted_reg_out_Mean_har_n211_top50.npy'))\n",
    "delta_grad1_TD = np.load(join(path_data, 'review/Top50_main/DC', 'Delta_grad1_TD_Surf_pearson_sorted_reg_out_Mean_har_n211_top50.npy'))\n",
    "delta_grad2_TD = np.load(join(path_data, 'review/Top50_main/DC', 'Delta_grad2_TD_Surf_pearson_sorted_reg_out_Mean_har_n211_top50.npy'))\n",
    "delta_grad3_TD = np.load(join(path_data, 'review/Top50_main/DC', 'Delta_grad3_TD_Surf_pearson_sorted_reg_out_Mean_har_n211_top50.npy'))\n",
    "\n",
    "delta_degree12 = np.load(join(path_data, 'review/Top50_main/DC', 'Delta_degree12_Surf_pearson_sorted_reg_out_Mean_har_n211_top50.npy'))\n",
    "delta_degree13 = np.load(join(path_data, 'review/Top50_main/DC', 'Delta_degree13_Surf_pearson_sorted_reg_out_Mean_har_n211_top50.npy'))\n",
    "delta_degree23 = np.load(join(path_data, 'review/Top50_main/DC', 'Delta_degree23_Surf_pearson_sorted_reg_out_Mean_har_n211_top50.npy'))\n",
    "delta_degree1_TD = np.load(join(path_data, 'review/Top50_main/DC', 'Delta_degree1_TD_Surf_pearson_sorted_reg_out_Mean_har_n211_top50.npy'))\n",
    "delta_degree2_TD = np.load(join(path_data, 'review/Top50_main/DC', 'Delta_degree2_TD_Surf_pearson_sorted_reg_out_Mean_har_n211_top50.npy'))\n",
    "delta_degree3_TD = np.load(join(path_data, 'review/Top50_main/DC', 'Delta_degree3_TD_Surf_pearson_sorted_reg_out_Mean_har_n211_top50.npy'))\n",
    "\n",
    "# group by cortex (Showing the largest group differences)\n",
    "delta_grad12_cortex = np.array([delta_grad12[: , [6, 17, 21, 162]].mean(axis=1), delta_grad12[: , [190,191]].mean(axis=1)]).T\n",
    "delta_degree12_cortex = np.array([delta_degree12[: , [6, 17, 21, 162]].mean(axis=1), delta_degree12[: , [190,191]].mean(axis=1)]).T\n",
    "\n",
    "delta_grad13_cortex = np.array([delta_grad13[: , [96]].mean(axis=1), delta_grad13[: , [318]].mean(axis=1)]).T\n",
    "delta_degree13_cortex = np.array([delta_degree13[: , [96]].mean(axis=1), delta_degree13[: , [318]].mean(axis=1)]).T\n",
    "\n",
    "delta_grad23_cortex = np.array([delta_grad23[: , [49]].mean(axis=1), delta_grad23[: , [308]].mean(axis=1)]).T\n",
    "delta_degree23_cortex = np.array([delta_degree23[: , [49]].mean(axis=1), delta_degree23[: , [308]].mean(axis=1)]).T\n",
    "\n",
    "\n",
    "delta_grad1_TD_cortex = np.array([delta_grad1_TD[: , [6,17,21,152,153,159,162]].mean(axis=1), delta_grad1_TD[: , [240, 241, 243, 244, 248, 267, 343, 344, 359]].mean(axis=1)]).T\n",
    "delta_degree1_TD_cortex = np.array([delta_degree1_TD[: , [6,17,21,152,153,159,162]].mean(axis=1), delta_degree1_TD[: , [240, 241, 243, 244, 248, 267, 343, 344, 359]].mean(axis=1)]).T\n",
    "\n",
    "delta_grad2_TD_cortex = np.array([delta_grad2_TD[: , [49]].mean(axis=1), delta_grad2_TD[: , [190]].mean(axis=1)]).T\n",
    "delta_degree2_TD_cortex = np.array([delta_degree2_TD[: , [49]].mean(axis=1), delta_degree2_TD[: , [190]].mean(axis=1)]).T\n",
    "\n",
    "delta_grad3_TD_cortex = np.array([delta_grad3_TD[: , [198]].mean(axis=1), delta_grad3_TD[: , [198]].mean(axis=1)]).T\n",
    "delta_degree3_TD_cortex = np.array([delta_degree3_TD[: , [198]].mean(axis=1), delta_degree3_TD[: , [198]].mean(axis=1)]).T\n",
    "\n",
    "# Plot\n",
    "for i in range(6):\n",
    "    grad_list = [delta_grad1_TD_cortex, delta_grad2_TD_cortex, delta_grad3_TD_cortex, delta_grad12_cortex, delta_grad13_cortex, delta_grad23_cortex]\n",
    "    degree_list = [delta_degree1_TD_cortex, delta_degree2_TD_cortex, delta_degree3_TD_cortex, delta_degree12_cortex, delta_degree13_cortex, delta_degree23_cortex]\n",
    "\n",
    "    plot_grad_DC_corr(grad_list[i], degree_list[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
